{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T06:40:26.417400Z",
     "iopub.status.busy": "2023-07-13T06:40:26.417242Z",
     "iopub.status.idle": "2023-07-13T06:40:26.428582Z",
     "shell.execute_reply": "2023-07-13T06:40:26.427990Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T06:40:26.430818Z",
     "iopub.status.busy": "2023-07-13T06:40:26.430522Z",
     "iopub.status.idle": "2023-07-13T06:40:27.436429Z",
     "shell.execute_reply": "2023-07-13T06:40:27.435960Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import os\n",
    "\n",
    "import graph_tool.all as gt\n",
    "import matplotlib\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import hypergeom, pearsonr\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from functions import *\n",
    "\n",
    "\n",
    "# Graph-Tool compatibility\n",
    "plt.switch_backend('cairo')\n",
    "# Style\n",
    "sns.set_theme(context='talk', style='white', palette='Set2')\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/nck/repos/GNN-Plus/Attention Analysis/scripts/functions/file.py:122: DtypeWarning: Columns (5,157,158,159,161,164,165,166,167,170,172,176,177,178,179,189,191,193,195,276,285,286,287,288,289,290,291,292,293,294,295,297,342,345,346,462,465,566,570,571,572,573,574,580,582,583,584,586,589,592,594,597,599,601,603,606,607,611,613,615,617,618,620,622,625,684,686) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(META)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available attention columns: ['att_D_AD_1', 'att_D_AD_2', 'att_D_SCZ_1', 'att_D_SCZ_2', 'att_D_no_prior_0', 'att_D_no_prior_1', 'att_D_no_prior_2', 'att_D_no_prior_3']\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "meta = get_meta()\n",
    "\n",
    "# Subject preview\n",
    "filtered = []\n",
    "for i, row in meta.iterrows():\n",
    "    try:\n",
    "        load_graph_by_id(row['SubID'])\n",
    "        assert not np.isnan(row['nps_MoodDysCurValue'])  # Has NPS information available\n",
    "        assert row['BRAAK_AD'] in (6,) and row['CERAD'] in (4,) and row['CDRScore'] in (3,)\n",
    "    except:\n",
    "        continue\n",
    "    filtered.append(f'{row[\"SubID\"]} {row[\"Ethnicity\"]} {row[\"Sex\"]}, {row[\"Age\"]}, BRAAK {row[\"BRAAK_AD\"]}, CERAD {row[\"CERAD\"]}, CDR {row[\"CDRScore\"]}, {row[\"Dx\"]}')\n",
    "filtered = np.sort(filtered)\n",
    "for i in range(len(filtered)):\n",
    "    # print(filtered[i])\n",
    "    pass\n",
    "\n",
    "# Parameters\n",
    "print(f'\\nAvailable attention columns: {get_attention_columns()}')\n",
    "column_ad = get_attention_columns()[0]\n",
    "column_scz = get_attention_columns()[2]\n",
    "column_data = get_attention_columns()[4]\n",
    "synthetic_nodes_of_interest = ['OPC', 'Micro', 'Oligo']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './attentions.pkl'\n",
    "if os.path.isfile(fname):\n",
    "    # Load data\n",
    "    with open('./attentions.pkl', 'rb') as f:\n",
    "        all_data = pickle.load(f)\n",
    "    attention_stack, all_edges, columns, subject_ids = all_data['data'], all_data['edges'], all_data['heads'], all_data['subject_ids']\n",
    "\n",
    "else:\n",
    "    # Parameters\n",
    "    # Scaled probably shouldn't be used, but better for visualization\n",
    "    # until results are more even\n",
    "    columns = get_attention_columns(scaled=False)\n",
    "    subject_ids = meta['SubID'].to_numpy()\n",
    "\n",
    "    # Load graphs\n",
    "    graphs, subject_ids = load_many_graphs(subject_ids, column=columns)\n",
    "    # graphs = [compute_graph(g) for g in graphs]\n",
    "\n",
    "    # # Get attentions\n",
    "    # df = {}\n",
    "    # for column in get_attention_columns():\n",
    "    #     attention, _ = compute_edge_summary(graphs, subject_ids=subject_ids)\n",
    "    #     attention = attention.set_index('Edge')\n",
    "    #     df[column] = attention.var(axis=1)\n",
    "\n",
    "\n",
    "    # Set indices to edges and clean\n",
    "    print('Fixing indices...')\n",
    "    for i in tqdm(range(len(graphs))):\n",
    "        graphs[i].index = graphs[i].apply(lambda r: get_edge_string([r['TF'], r['TG']]), axis=1)\n",
    "        graphs[i] = graphs[i].drop(columns=['TF', 'TG'])\n",
    "        # Remove duplicates\n",
    "        graphs[i] = graphs[i][~graphs[i].index.duplicated(keep='first')]\n",
    "\n",
    "    # Get all unique edges\n",
    "    print('Getting unique edges...')\n",
    "    all_edges = np.unique(sum([list(g.index) for g in graphs], []))\n",
    "\n",
    "\n",
    "    # Standardize index order\n",
    "    print('Standardizing indices...')\n",
    "    for i in tqdm(range(len(graphs))):\n",
    "        # Add missing indices and order based on `all_edges`\n",
    "        # to_add = [edge for edge in all_edges if edge not in list(graphs[i].index)]  # SLOW\n",
    "        to_add = list(set(all_edges) - set(graphs[i].index))\n",
    "\n",
    "        # Empty rows\n",
    "        new_rows = pd.DataFrame(\n",
    "            [[np.nan]*len(graphs[i].columns)]*len(to_add),\n",
    "            columns=graphs[i].columns,\n",
    "        ).set_index(pd.Series(to_add))\n",
    "        # Native concat\n",
    "        graphs[i] = pd.concat([graphs[i], new_rows]).loc[all_edges]\n",
    "\n",
    "    # Convert to numpy\n",
    "    graphs = [g.to_numpy() for g in graphs]\n",
    "    attention_stack = np.stack(graphs, axis=-1)\n",
    "    # attention_stack.shape = (Edge, Head, Subject)\n",
    "    # attention_stack.shape = (all_edges, columns, subject_ids)\n",
    "\n",
    "    # Save all data\n",
    "    all_data = {'data': attention_stack, 'edges': all_edges, 'heads': columns, 'subject_ids': subject_ids}\n",
    "    # np.savez('attentions.npz', **all_data)\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(\n",
    "            all_data,\n",
    "            f,\n",
    "            protocol=pickle.HIGHEST_PROTOCOL,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional useful parameters\n",
    "self_loops = [split_edge_string(s)[0] == split_edge_string(s)[1] for s in all_edges]\n",
    "self_loops = np.array(self_loops)\n",
    "# Remove self loops\n",
    "all_edges = all_edges[~self_loops]\n",
    "attention_stack = attention_stack[~self_loops]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Comparisons (Figure 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_comparisons = [\n",
    "    # (subject_id_1, subject_id_2, column)\n",
    "    # for subject_id_1, subject_id_2, column in individual_comparisons:\n",
    "    # ('M19050', 'M59593', column_ad),  # Diff ancestry and Dx (Vascular)\n",
    "    # ('M19050', 'M59593', column_data),\n",
    "    ('M35115', 'M35594', column_ad),  # EUR Female 88, BRAAK 6, CERAD 4, CDR 3, AD-Def - EUR Female 90+, BRAAK 6, CERAD 4, CDR 3, AD-Def\n",
    "    ('M35115', 'M35594', column_data),\n",
    "]\n",
    "palette = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "individual_colors = {\n",
    "    sid: rgba_to_hex(palette[i]) for i, sid in enumerate(\n",
    "        sum([list(comparison[:2]) for comparison in individual_comparisons], []))\n",
    "}\n",
    "\n",
    "# Verify all are available\n",
    "for subject_id_1, subject_id_2, column in individual_comparisons:\n",
    "    for sid in [subject_id_1, subject_id_2]:\n",
    "        load_graph_by_id(sid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A Mini Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M35115 - M35594 - att_D_AD_1\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 378/378 [00:00<00:00, 311421.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating positions...\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 171/171 [00:00<00:00, 299343.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 207/207 [00:00<00:00, 336128.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_no_prior_0\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 378/378 [00:00<00:00, 328046.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating positions...\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 171/171 [00:00<00:00, 318484.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 207/207 [00:00<00:00, 268799.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (subject_id_1, subject_id_2, column) in enumerate(individual_comparisons):\n",
    "    print(' - '.join((subject_id_1, subject_id_2, column)))\n",
    "\n",
    "    # Assemble\n",
    "    sids = [subject_id_1, subject_id_2]\n",
    "    gs = [compute_graph(load_graph_by_id(sid, column=column)) for sid in sids]\n",
    "\n",
    "    # Filter\n",
    "    gs = [\n",
    "        filter_to_synthetic_vertices(g.copy(), vertex_ids=synthetic_nodes_of_interest)\n",
    "        for g in gs\n",
    "    ]\n",
    "\n",
    "    # Recalculate\n",
    "    gs = [assign_vertex_properties(g) for g in gs]\n",
    "\n",
    "    # Plot\n",
    "    fig, axs = get_mosaic([list(range(2))], scale=9)\n",
    "    plot_graph_comparison(gs, axs=axs, subject_ids=sids)\n",
    "    fig.savefig(f'../plots/individual_mini_{\"-\".join(sids)}_{column}.pdf', format='pdf', transparent=True, backend='cairo')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B Attention Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M35115 - M35594 - att_D_AD_1\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 344156.29it/s]\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_AD_2\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 331738.52it/s]\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_SCZ_1\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 353863.58it/s]\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_SCZ_2\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 318704.49it/s]\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_no_prior_0\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 277121.79it/s]\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_no_prior_1\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 329591.72it/s]\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_no_prior_2\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 328580.15it/s]\n",
      "/mnt/c/Users/nck/repos/GNN-Plus/Attention Analysis/scripts/functions/plotting.py:476: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = plt.figure(figsize=(scale*len(mosaic[0]), scale*len(mosaic)), constrained_layout=True)\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_no_prior_3\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 334713.94it/s]\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_AD_1\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 329313.87it/s]\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_AD_2\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 323115.71it/s]\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_SCZ_1\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 321185.44it/s]\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_SCZ_2\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 320890.74it/s]\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_no_prior_0\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 338097.72it/s]\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_no_prior_1\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 332206.70it/s]\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_no_prior_2\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 317239.72it/s]\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "M35115 - M35594 - att_D_no_prior_3\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 326070.94it/s]\n",
      "/tmp/ipykernel_313/1097658869.py:16: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for subject_id_1, subject_id_2, _ in individual_comparisons:\n",
    "    for column in get_attention_columns():\n",
    "        print(' - '.join((subject_id_1, subject_id_2, column)))\n",
    "\n",
    "        # Assemble\n",
    "        sample_ids = [subject_id_1, subject_id_2]\n",
    "        graphs = [compute_graph(load_graph_by_id(sid, column=column)) for sid in sample_ids]\n",
    "\n",
    "        # Get graph\n",
    "        g = concatenate_graphs(*graphs, threshold=False)\n",
    "        g = get_intersection(g)\n",
    "        g = cull_isolated_leaves(g)\n",
    "\n",
    "        fig, axs = get_mosaic([list(range(1))], scale=6)\n",
    "        df = plot_individual_edge_comparison(g, sample_ids, color_map=individual_colors, ax=axs[0])\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(f'../plots/individual_edge_comparison_{\"-\".join((subject_id_1, subject_id_2))}_{column}.pdf', format='pdf', transparent=True, backend='cairo')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3C Module Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thema/miniconda3/envs/GNN/lib/python3.11/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/thema/miniconda3/envs/GNN/lib/python3.11/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for subject_id_1, subject_id_2, column in individual_comparisons:\n",
    "    # Get graphs\n",
    "    g1 = compute_graph(load_graph_by_id(subject_id_1, column=column))\n",
    "    g2 = compute_graph(load_graph_by_id(subject_id_2, column=column))\n",
    "\n",
    "    # Compute module scores\n",
    "    def get_module_scores(g):\n",
    "        association = []\n",
    "        name = []\n",
    "        score = []\n",
    "        for v in g.vertices():\n",
    "            # Escape if not TF\n",
    "            if 'tf' not in g.vp.node_type[v]: continue\n",
    "            # Get association\n",
    "            association_list = None\n",
    "            for e in v.out_edges():\n",
    "                v_source = e.target()\n",
    "                # If synthetic, record\n",
    "                if 'celltype' == g.vp.node_type[v_source]:\n",
    "                    if association_list is None: association_list = [g.vp.ids[v_source]]\n",
    "                    else: association_list += [g.vp.ids[v_source]]\n",
    "\n",
    "            # Get scores\n",
    "            for e in v.out_edges():\n",
    "                v_target = e.target()\n",
    "                # Escape if not TG\n",
    "                if 'tg' not in g.vp.node_type[v_target]: continue\n",
    "                # Record weights\n",
    "                for assoc in association_list:\n",
    "                    association.append(assoc)\n",
    "                    name.append(g.vp.ids[v])\n",
    "                    score.append(g.ep.coef[e])\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'Cell Type': association,\n",
    "            'TF': name,\n",
    "            'Module Score': score,\n",
    "        }).assign(TGs=1).groupby(['Cell Type', 'TF']).sum().reset_index()\n",
    "\n",
    "        # Append #TGs to TF names\n",
    "        df['TF'] = df.apply(lambda r: f'{r[\"TF\"]} ({r[\"TGs\"]})', axis=1)\n",
    "        df = df.drop(columns='TGs')\n",
    "\n",
    "        return df\n",
    "    # Get module scores\n",
    "    module_scores_1 = get_module_scores(g1)\n",
    "    module_scores_2 = get_module_scores(g2)\n",
    "    # Make blanks\n",
    "    zeros_1 = module_scores_1.copy()\n",
    "    zeros_1['Module Score'] = 0\n",
    "    zeros_2 = module_scores_2.copy()\n",
    "    zeros_2['Module Score'] = 0\n",
    "    # Append for consistency\n",
    "    module_scores_1 = pd.concat((module_scores_1, zeros_2)).groupby(['Cell Type', 'TF']).max().reset_index()\n",
    "    module_scores_2 = pd.concat((module_scores_2, zeros_1)).groupby(['Cell Type', 'TF']).max().reset_index()\n",
    "    # Concatenate subjects\n",
    "    # NOTE: Only matters that they're in the order sub_1 -> sub_2\n",
    "    # and all present for the `.diff()` groupby, no need to label\n",
    "    # module_scores_1['Subject'] = subject_id_1\n",
    "    # module_scores_2['Subject'] = subject_id_2\n",
    "    module_scores = pd.concat((module_scores_1, module_scores_2))\n",
    "    module_scores['Module Score'] = module_scores.groupby(['Cell Type', 'TF'])['Module Score'].diff(periods=-1)  # First minus second\n",
    "    module_scores = module_scores.loc[~module_scores['Module Score'].isna()]\n",
    "\n",
    "    # Filter to only high values\n",
    "    # module_scores = module_scores.loc[module_scores['Module Score'] > .1]\n",
    "\n",
    "    # Plot\n",
    "    fig, axs = get_mosaic([[0]*2], scale=6)\n",
    "    import colorsys\n",
    "    def plot_module_scores(module_scores, ax=None):\n",
    "        # Pivot\n",
    "        df = module_scores.pivot(index='Cell Type', columns='TF', values='Module Score')\n",
    "        # Roughly sort by cell type\n",
    "        df = df.T\n",
    "        for c in df.columns:\n",
    "            df = df.sort_values(c)\n",
    "        df = df.T  # .iloc[::-1]\n",
    "        # Plot\n",
    "        from matplotlib.colors import SymLogNorm\n",
    "        pl = sns.heatmap(\n",
    "            data=df,\n",
    "            vmin=np.abs(df.fillna(0).to_numpy()).max(),\n",
    "            vmax=-np.abs(df.fillna(0).to_numpy()).max(),\n",
    "            norm=SymLogNorm(linthresh=1),\n",
    "            cmap=sns.diverging_palette(\n",
    "                360*colorsys.rgb_to_hls(*hex_to_rgb(individual_colors[subject_id_2]))[0],\n",
    "                360*colorsys.rgb_to_hls(*hex_to_rgb(individual_colors[subject_id_1]))[0],\n",
    "                s=70,\n",
    "                l=60,\n",
    "                center='dark',\n",
    "                as_cmap=True),\n",
    "            cbar_kws={'label': f'{subject_id_2} - Module Prioritization - {subject_id_1}'},\n",
    "            ax=ax)\n",
    "        pl.set(xlabel=f'TF (+) (n={df.shape[0]})')\n",
    "        return pl\n",
    "    p1 = plot_module_scores(module_scores, ax=axs[0])\n",
    "\n",
    "    # Inset axis\n",
    "    from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "    axins = inset_axes(\n",
    "        axs[0],\n",
    "        width='30%', height='15%',\n",
    "        loc=4,\n",
    "        bbox_to_anchor=(.05, .05, 1, 1), bbox_transform=axs[0].transAxes)\n",
    "    # Take absolute module score for histogram, unevenly distributed\n",
    "    # TODO: Use symlog\n",
    "    module_scores_abs = module_scores.copy()\n",
    "    module_scores_abs['Module Score'] = module_scores_abs['Module Score'].abs()\n",
    "    sns.histplot(data=module_scores, x='Module Score', log_scale=True, kde=True, ax=axins)\n",
    "    plt.ylabel(None)\n",
    "    plt.xlabel(None)\n",
    "\n",
    "    # Format\n",
    "    p1.set(title=column)\n",
    "\n",
    "    # Save\n",
    "    fig.savefig(f'../plots/individual_module_analysis_{subject_id_1}_{subject_id_2}_{column}.pdf', format='pdf', transparent=True, backend='cairo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Module Discovery Barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject_id_1, subject_id_2, column in individual_comparisons:\n",
    "    # NOTE: Column doesn't matter here with the current snipping method\n",
    "    g1 = compute_graph(load_graph_by_id(subject_id_1, column=column))\n",
    "    g2 = compute_graph(load_graph_by_id(subject_id_2, column=column))\n",
    "\n",
    "    # Get unique TFs\n",
    "    df = compare_graphs_enrichment(\n",
    "        g1, g2,\n",
    "        sid_1=subject_id_1, sid_2=subject_id_2,\n",
    "        nodes=list(set(get_all_synthetic_ids(g1)).union(set(get_all_synthetic_ids(g2)))),  #list(set(get_all_synthetic_ids(g1)).intersection(set(get_all_synthetic_ids(g2)))),\n",
    "        include_tgs=True,\n",
    "        threshold=.5)\n",
    "\n",
    "    # Get counts of unique TFs\n",
    "    df = df.melt(var_name='String', value_name='Gene')\n",
    "    df['String'] = df['String'].apply(lambda x: x.split('.'))\n",
    "    df = pd.concat((pd.DataFrame(df['String'].tolist(), columns=('Subject', 'Cell Type')), df[['Gene']]), axis=1)\n",
    "    df = df.groupby(['Subject', 'Cell Type']).count().reset_index().rename(columns={'Gene': 'Unique Genes'})\n",
    "\n",
    "    # Filter to nodes of interest\n",
    "    df = df.loc[[ct in synthetic_nodes_of_interest for ct in df['Cell Type']]]\n",
    "    if df.shape[0] == 0: continue\n",
    "\n",
    "    # Plot\n",
    "    fig, axs = get_mosaic([[0]], scale=6)\n",
    "    pl = sns.barplot(\n",
    "        data=df,\n",
    "        x='Cell Type',\n",
    "        y='Unique Genes',\n",
    "        hue='Subject',\n",
    "        palette=individual_colors,\n",
    "        ax=axs[0])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(column)\n",
    "    fig.savefig(f'../plots/individual_module_discovery_barplot_{subject_id_1}_{subject_id_2}_{column}.pdf', format='pdf', transparent=True, backend='cairo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3E Edge Discovery Line Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "percentage_prioritizations_range = (.05, .06)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold by max/10 on head\n",
    "# NOTE: Percentile is still 0 at 99%\n",
    "head_threshold = np.nan_to_num(attention_stack).max(axis=(0, 2)).reshape((1, -1, 1)) / 10\n",
    "within_range = attention_stack > head_threshold\n",
    "\n",
    "# Get counts for edges\n",
    "counts = within_range.sum(axis=2)\n",
    "counts = pd.DataFrame(counts, index=all_edges, columns=columns)\n",
    "\n",
    "# Melt and format\n",
    "counts = counts.reset_index(names='Edge').melt(id_vars='Edge', var_name='Head', value_name='Count')\n",
    "\n",
    "# Remove low counts (was zero, but far too many were low)\n",
    "counts = counts.loc[counts['Count'] > 1]\n",
    "\n",
    "# # Average plot\n",
    "# # Sort by highest spike\n",
    "# counts = counts.sort_values('Count')\n",
    "# # Plot\n",
    "# fig, axs = get_mosaic([[0]*2], scale=6)\n",
    "# pl = sns.lineplot(data=counts, x='Edge', y='Count', hue='Head')\n",
    "# plt.xticks(rotation=90)\n",
    "# # plt.yscale('log')\n",
    "# limit_labels(pl, n=10)\n",
    "# fig.savefig(f'../plots/individual_edge_discovery_lineplot.pdf', format='pdf', transparent=True, backend='cairo')\n",
    "\n",
    "for column in columns:\n",
    "    # Filter to column\n",
    "    counts_filtered = counts.loc[counts['Head']==column]\n",
    "\n",
    "    # Sample\n",
    "    # NOTE: Maybe remove in final version?  Doesn't matter too much\n",
    "    np.random.seed(42)\n",
    "    idx = np.random.choice(counts_filtered.shape[0], min(1_000, counts_filtered.shape[0]), replace=False)\n",
    "    counts_filtered = counts_filtered.iloc[idx]\n",
    "\n",
    "    # Sort\n",
    "    counts_filtered = counts_filtered.sort_values('Count')\n",
    "\n",
    "    # Plot\n",
    "    fig, axs = get_mosaic([[0, 0]], scale=6)\n",
    "    pl = sns.lineplot(data=counts_filtered, x='Edge', y='Count')\n",
    "\n",
    "    # Highlight area\n",
    "    axs[0].axhspan(\n",
    "        percentage_prioritizations_range[0]*attention_stack.shape[2],\n",
    "        percentage_prioritizations_range[1]*attention_stack.shape[2],\n",
    "        color='red', alpha=.2, lw=0)\n",
    "\n",
    "    # Format\n",
    "    plt.xticks(rotation=60)\n",
    "    pl.set(title=column)\n",
    "    # plt.yscale('log')\n",
    "    limit_labels(pl, n=20)\n",
    "\n",
    "    # Save\n",
    "    fig.savefig(f'../plots/individual_edge_discovery_lineplot_{column}.pdf', format='pdf', transparent=True, backend='cairo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3F Edge Discovery Summary Barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine edges that are highly individual for enrichment (between `percentile_prioritizations_range`%s)\n",
    "individual_genes = counts.loc[(counts['Count'] > (percentage_prioritizations_range[0]*attention_stack.shape[2])) * (counts['Count'] < (percentage_prioritizations_range[1]*attention_stack.shape[2]))]\n",
    "individual_genes = individual_genes.copy()\n",
    "# Parse into genes from edges\n",
    "individual_genes['Edge'] = individual_genes['Edge'].map(lambda s: split_edge_string(s))\n",
    "individual_genes = individual_genes.drop(columns='Edge').reset_index(drop=True).join(pd.DataFrame(individual_genes['Edge'].to_list(), columns=('TF', 'TG')))\n",
    "individual_genes = individual_genes.melt(id_vars=['Head', 'Count'], var_name='Gene Type', value_name='Gene').drop(columns=['Gene Type', 'Count']).drop_duplicates()\n",
    "# Filter synthetic\n",
    "individual_genes = individual_genes.loc[individual_genes['Gene'].apply(lambda s: not string_is_synthetic(s))]\n",
    "\n",
    "# Plot\n",
    "fig, axs = get_mosaic([[0]], scale=6)\n",
    "barplot_individual_genes = individual_genes.groupby('Head').count().reset_index().rename(columns={'Gene': 'Unique Genes'})\n",
    "pl = sns.barplot(data=barplot_individual_genes, x='Head', y='Unique Genes')\n",
    "plt.xticks(rotation=90)\n",
    "# plt.yscale('log')\n",
    "fig.savefig(f'../plots/individual_edge_discovery_summary_barplot.pdf', format='pdf', transparent=True, backend='cairo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3G Pathway Enrichment (MANUAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject_id_1, subject_id_2, column in individual_comparisons:\n",
    "    # NOTE: Column doesn't matter here with the current snipping method\n",
    "    g1 = compute_graph(load_graph_by_id(subject_id_1, column=column))\n",
    "    g2 = compute_graph(load_graph_by_id(subject_id_2, column=column))\n",
    "\n",
    "    # Get unique modules\n",
    "    df = compare_graphs_enrichment(g1, g2, sid_1=subject_id_1, sid_2=subject_id_2, nodes=synthetic_nodes_of_interest, threshold=.5)\n",
    "\n",
    "    # Add individually important edges (requires above)\n",
    "    for column in np.unique(individual_genes['Head']):\n",
    "        df_new = pd.DataFrame(individual_genes.loc[individual_genes['Head']==column, 'Gene'].to_list(), columns=(f'{column}',))\n",
    "        df = df.join(df_new, how='outer')\n",
    "\n",
    "    # Save to file\n",
    "    df.to_csv(f'../plots/genes_{subject_id_1}_{subject_id_2}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrichment\n",
    "for subject_id_1, subject_id_2, column in individual_comparisons:\n",
    "    # MANUAL PROCESSING\n",
    "    # Run the output from above on Metascape as multiple gene list and perform\n",
    "    # enrichment.  From the all-in-one ZIP file, save the file from\n",
    "    # Enrichment_QC/GO_DisGeNET.csv as '../plot/disgenet_{subject_id_1}_{subject_id_2}_{column}.csv' and\n",
    "    # Overlap_circos/CircosOverlapByGene.svg as '../plot/overlap_{subject_id_1}_{subject_id_2}_{column}.svg'\n",
    "\n",
    "    # Get enrichment\n",
    "    enrichment_file = f'../plots/disgenet_{subject_id_1}_{subject_id_2}.csv'\n",
    "    if not os.path.isfile(enrichment_file): continue\n",
    "    enrichment = pd.read_csv(enrichment_file)\n",
    "\n",
    "    # Format\n",
    "    enrichment = format_enrichment(enrichment, filter=15)\n",
    "\n",
    "    # Filter to certain groups\n",
    "    # TODO: Fix these groups in previous section\n",
    "    gene_sets = np.unique(enrichment['Gene Set'])[[0, 1, 2, 3, 4, 5, 6, 8, 10]]\n",
    "    enrichment = enrichment.loc[enrichment['Gene Set'].apply(lambda s: s in gene_sets)]\n",
    "\n",
    "    # Plot\n",
    "    fig, axs = get_mosaic([[0]*2], scale=12)\n",
    "    pl = sns.scatterplot(\n",
    "        enrichment,\n",
    "        x='Description', y='Gene Set',\n",
    "        size='-log10(p)',\n",
    "        color='black',\n",
    "        ax=axs[0])\n",
    "    # Formatting\n",
    "    pl.grid()\n",
    "    plt.xticks(rotation=90)\n",
    "    pl.set_aspect('equal', 'box')\n",
    "    pl.legend(title='-log10(p)', bbox_to_anchor=(1.8, .5))\n",
    "    # Zoom X\n",
    "    margin = .5\n",
    "    min_xlim, max_xlim = pl.get_xlim()\n",
    "    min_xlim -= margin; max_xlim += margin\n",
    "    pl.set(xlim=(min_xlim, max_xlim))\n",
    "    fig.savefig(f'../plots/individual_enrichment_{subject_id_1}_{subject_id_2}.pdf', format='pdf', transparent=True, backend='cairo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3X Head Variation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate heatmap\n",
    "# df = np.var(np.nan_to_num(attention_stack), axis=2)\n",
    "# # Create df\n",
    "# df = pd.DataFrame(df, index=all_edges, columns=columns)\n",
    "\n",
    "# # Sort\n",
    "# # df = df.iloc[df.fillna(0).mean(axis=1).argsort().to_numpy()[::-1]]\n",
    "# # Standardize for visualization\n",
    "# # TODO: Remove once model scale is fixed, only for visualization\n",
    "# df = df / df.max(axis=0)\n",
    "\n",
    "# ### Combined clustermap\n",
    "# # Assign groups by associated cell type\n",
    "# # TODO: Make greater depth, currently 1\n",
    "# clusters = pd.DataFrame(\n",
    "#     np.array([\n",
    "#         [tf, tg] for tf, tg in df.index.map(lambda s: split_edge_string(s))\n",
    "#     ]),\n",
    "#     index=df.index,\n",
    "#     columns=pd.Series(['TF', 'TG']),\n",
    "# )\n",
    "# assign_saved = {}\n",
    "# def assign(row, df=None):\n",
    "#     # Progress printing\n",
    "#     # if np.random.rand() < .01:\n",
    "#     #     print(f'{row[\"TF\"]} - {row[\"TG\"]}')\n",
    "\n",
    "#     # If directly related\n",
    "#     tf_synthetic = string_is_synthetic(row['TF'])\n",
    "#     tg_synthetic = string_is_synthetic(row['TG'])\n",
    "#     if tf_synthetic and tg_synthetic and (row['TF'] != row['TG']):\n",
    "#         return 'Multiple'\n",
    "#     elif tf_synthetic:\n",
    "#         return row['TF']\n",
    "#     elif tg_synthetic:\n",
    "#         return row['TG']\n",
    "\n",
    "#     # Otherwise, take indirect associations\n",
    "#     if df is not None and 'Association' in df:\n",
    "#         # Default to TF association\n",
    "#         if row['TF'] not in assign_saved:\n",
    "#             nodes, counts = np.unique(df.loc[df['TF']==row['TF'], 'Association'], return_counts=True)\n",
    "#             nodes, counts = nodes[nodes!='None'], counts[nodes!='None']\n",
    "#             if nodes.shape[0] == 0: assign_saved[row['TF']] = 'None'\n",
    "#             else: assign_saved[row['TF']] = nodes[np.argsort(counts)[::-1]][0]\n",
    "#         return assign_saved[row['TF']]\n",
    "\n",
    "#     # If all else fails, return no association\n",
    "#     return 'None'\n",
    "\n",
    "# # Propagate cell types\n",
    "# for _ in range(2):  # Depth 2\n",
    "#     clusters['Association'] = clusters.apply(lambda x: assign(x, df=clusters), axis=1)\n",
    "\n",
    "# # Convert to colors\n",
    "# cluster_colors = {\n",
    "#     a: c for a, c in zip(\n",
    "#         np.unique(clusters['Association']),\n",
    "#         sns.color_palette(palette='husl', n_colors=np.unique(clusters['Association']).shape[0]),\n",
    "#     )\n",
    "# }\n",
    "# clusters['Colors'] = clusters['Association'].apply(lambda a: cluster_colors[a])\n",
    "\n",
    "# # Filter to top 10 per head\n",
    "# idx = []\n",
    "# for column in columns:\n",
    "#     idx += list(df.sort_values(column).index[-10:])\n",
    "# idx = np.unique(idx)\n",
    "# df = df.loc[idx]\n",
    "# clusters = clusters.loc[idx]\n",
    "\n",
    "# # Plot\n",
    "# np.random.seed(42)\n",
    "# fig = sns.clustermap(\n",
    "#     data=df,\n",
    "#     row_colors=clusters[['Colors']].rename(columns={'Colors': 'Cell Association'}),\n",
    "#     row_cluster=False,\n",
    "#     # norm=LogNorm(),\n",
    "#     cmap='mako_r',\n",
    "#     # dendrogram_ratio=.1,\n",
    "#     # cbar_kws={'label': 'Variation'}\n",
    "#     figsize=(9, 27),\n",
    "# )\n",
    "# fig.savefig(f'../plots/individual_edge_variance_heatmap.pdf', format='pdf', transparent=True, backend='cairo')\n",
    "# plt.show()\n",
    "# # Plot legend\n",
    "# plt.clf()\n",
    "# ax = plt.gca()\n",
    "# legend_elements = [\n",
    "#     Line2D([0], [0], color='gray', linestyle='None', markersize=10, marker='s', markerfacecolor=color, label=ct)\n",
    "#     for ct, color in cluster_colors.items()\n",
    "# ]\n",
    "# ax.legend(handles=legend_elements, loc='best')\n",
    "# plt.gca().axis('off')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(f'../plots/individual_edge_variance_heatmap_cell_legend.pdf', format='pdf', transparent=True, backend='cairo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3X Individual Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# column = column_data\n",
    "\n",
    "# # Filter to data column\n",
    "# data_idx = np.argwhere(np.array(columns) == column)[0][0]\n",
    "# df = pd.DataFrame(attention_stack[:, data_idx], index=all_edges, columns=subject_ids)\n",
    "# # Sort and filter (fillna can be excluded, but this also makes more common edges visible)\n",
    "# df = df.iloc[df.fillna(0).mean(axis=1).argsort().to_numpy()[::-1]]\n",
    "# df = df.iloc[:5000]\n",
    "# # Sort and filter by common edges\n",
    "# df = df.iloc[:, df.isna().to_numpy().sum(axis=0).argsort()]\n",
    "# df = df.iloc[:, :100]\n",
    "\n",
    "# # Individual heatmap (Limited to top 5k links)\n",
    "# fig, axs = get_mosaic([[0]*9]*9, scale=3)\n",
    "# sns.heatmap(data=df.iloc[:5000], cmap='mako_r', ax=axs[0])  # , norm=LogNorm()\n",
    "# plt.xticks(rotation=60)\n",
    "# fig.savefig(f'../plots/individual_edge_heatmap_{column}.pdf', format='pdf', transparent=True, backend='cairo')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3X Dosage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# column = column_ad\n",
    "# subject_ids = meta['SubID'].to_numpy()\n",
    "\n",
    "# # Load graphs\n",
    "# graphs, subject_ids = load_many_graphs(subject_ids, column=column)\n",
    "# graphs = [compute_graph(g) for g in graphs]\n",
    "\n",
    "# # Get dosage information\n",
    "# dosage = get_dosage()\n",
    "# # Why do some SNPs go missing with the new meta?\n",
    "# dosage = convert_dosage_ids_to_subject_ids(dosage, meta=meta)\n",
    "\n",
    "# # Get attention\n",
    "# attention, _ = compute_edge_summary(graphs, subject_ids=subject_ids)\n",
    "# attention = attention.set_index('Edge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select target SNP\n",
    "# target_snp = dosage.index[42]  # Random for now\n",
    "\n",
    "# # Make df\n",
    "# data_dosage = dosage.loc[[target_snp]].T\n",
    "# data_attention = attention.T\n",
    "# df = data_dosage.join(data_attention, how='inner')\n",
    "\n",
    "# # Select target edge\n",
    "# p_min = 1\n",
    "# for edge in attention.index:\n",
    "#     corr, pval = scipy.stats.pearsonr(\n",
    "#         df[[edge]].to_numpy().squeeze(),\n",
    "#         df[[target_snp]].to_numpy().squeeze())\n",
    "#     if pval < p_min:\n",
    "#         p_min = pval\n",
    "#         best_corr = corr\n",
    "#         target_edge = edge\n",
    "# print(f'Found minimal p-value of {p_min:.6f} (Correlation: {best_corr:.6f}).')\n",
    "\n",
    "# # Format df\n",
    "# axis_snp = f'{target_snp} Dosage'\n",
    "# axis_edge = f'{target_edge} Attention'\n",
    "# df = df.rename(columns={target_snp: axis_snp, target_edge: axis_edge})\n",
    "\n",
    "# # Scatter\n",
    "# fig, axs = get_mosaic([list(range(1))], scale=9)\n",
    "# sns.scatterplot(data=df, x=axis_snp, y=axis_edge, ax=axs[0])\n",
    "# fig.savefig(f'../plots/individual_dosage_correlation_{column}.pdf', format='pdf', transparent=True, backend='cairo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Comparisons (Figure 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinations\n",
    "# TODO: Potentially move each entry to dictionary, so changes in order\n",
    "#   are easier to propagate\n",
    "contrast_groupings = [\n",
    "    # (contrast name, contrast group, attention column, comparison column, target meta column, other target meta column)\n",
    "    # for contrast_name, contrast_group, column, comparison, target, target_comparison in contrast_groupings:\n",
    "    # TODO: Revise ethnicity prediction\n",
    "    ('c15x', 'AD', column_ad, column_data, 'BRAAK_AD', 'Ethnicity'),\n",
    "    ('c15x', 'AD', column_data, column_ad, 'BRAAK_AD', 'Ethnicity'),\n",
    "    ('c15x', 'AD', column_data, column_ad, 'nps_MoodDysCurValue', 'nps_WtGainCurValue'),\n",
    "    # ('c06x', 'AD', column_ad, column_data, 'BRAAK_AD', 'nps_MoodDysCurValue'),  # Eventually SCZ, BP and such\n",
    "    # ('c71x', 'MoodDys', column_data, column_ad, 'nps_MoodDysCurValue'),  # Dysphoria\n",
    "    # ('c72x', 'DecInt', column_data, column_ad, 'nps_DecIntCurValue'),  # Anhedonia\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4B Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for contrast_name, _, column, comparison, target, target_comparison in contrast_groupings:\n",
    "    # Filter attention stack to contrast\n",
    "    contrast = get_contrast(contrast_name)\n",
    "    contrast_subject_ids = sum([contrast[group] for group in contrast], [])\n",
    "    contrast_mask = [sid in contrast_subject_ids for sid in subject_ids]\n",
    "    contrast_subject_ids = np.array(subject_ids)[contrast_mask]\n",
    "    contrast_stack = attention_stack[:, :, contrast_mask]\n",
    "\n",
    "    # Filter to 1000 most variant edges\n",
    "    top_variant_edge_idx = np.nan_to_num(\n",
    "        contrast_stack[:, np.argwhere(np.array(columns)==column)[0][0]]).var(axis=1).argsort()[::-1][:1000]\n",
    "    contrast_stack = contrast_stack[top_variant_edge_idx]\n",
    "    edge_names = all_edges[top_variant_edge_idx]\n",
    "\n",
    "    # Correlation df\n",
    "    df = pd.DataFrame(\n",
    "        contrast_stack[:, np.argwhere(np.array(columns)==column)[0][0]],\n",
    "        index=pd.Series(all_edges[top_variant_edge_idx]),\n",
    "        columns=contrast_subject_ids).T\n",
    "    df = df.join(meta.set_index('SubID')[[target, target_comparison]]).reset_index(drop=True)\n",
    "    # Select edge which most cleanly separates `target`\n",
    "    # top_distinct_edge_idx = df.drop(target_comparison, axis=1).groupby(target).mean().var(axis=0).argsort()[-1]\n",
    "    # Select top 3 most correlating edges\n",
    "    edge_name = df.drop(target_comparison, axis=1).corr()[target].abs().drop(target).sort_values(ascending=False)[:3].index.to_numpy()\n",
    "    top_distinct_edge_idx = [np.argwhere(df.columns==edge)[0][0] for edge in edge_name]\n",
    "    contrast_stack = contrast_stack[top_distinct_edge_idx]\n",
    "\n",
    "    # Plot\n",
    "    fig, axs = get_mosaic(np.array(sum([[i]*2 for i in range(6)], [])).reshape((2, -1)), scale=5)\n",
    "    # axs[0].sharex(axs[1])\n",
    "    sns.despine()\n",
    "\n",
    "    for i in range(3):\n",
    "        # Filter\n",
    "        contrast_stack_i = contrast_stack[i]\n",
    "        edge_name_i = edge_name[i]\n",
    "\n",
    "        # Scale attention\n",
    "        # TODO: Remove once heads are balanced\n",
    "        contrast_stack_i = contrast_stack_i / np.nan_to_num(contrast_stack_i).max(axis=1).reshape((-1, 1))\n",
    "\n",
    "        # Format\n",
    "        df = pd.DataFrame(contrast_stack_i, index=pd.Series(columns), columns=contrast_subject_ids)\n",
    "        df = df.reset_index(names='Head').melt(id_vars='Head', var_name='Subject', value_name=edge_name_i).dropna()  # Melt\n",
    "        df = df.set_index('Subject').join(meta.set_index('SubID')[[target, target_comparison]]).reset_index()  # Join meta\n",
    "\n",
    "        # Filter to target heads\n",
    "        df = df.loc[df['Head'].apply(lambda s: s in (column, comparison))]\n",
    "\n",
    "        # Main target\n",
    "        p1 = sns.violinplot(data=df, x='Head', y=edge_name_i, hue=target, ax=axs[i])\n",
    "        p1.legend(title=target, bbox_to_anchor=(1.1, 1.05))\n",
    "        p1.set(xlabel=None, xticklabels=[])\n",
    "\n",
    "        # Comparison target\n",
    "        p2 = sns.violinplot(data=df, x='Head', y=edge_name_i, hue=target_comparison, ax=axs[i+3])\n",
    "        p2.legend(title=target_comparison, bbox_to_anchor=(1.1, 1.05))\n",
    "        plt.sca(p2)\n",
    "        plt.xticks(rotation=60)\n",
    "\n",
    "        # Get correlation p-values for targets (which must be numeric)\n",
    "        for j, tar in enumerate((target, target_comparison)):\n",
    "            for k, c in enumerate(np.unique(df['Head'])):\n",
    "                try:\n",
    "                    pval = pearsonr(df.loc[df['Head']==c, edge_name_i], df.loc[df['Head']==c, tar])[1]\n",
    "                    axs[i+3*j].text(k, axs[i+3*j].get_ylim()[0] - (.15 if not j else .3), f'p={pval:.1e}', ha='center', va='center')\n",
    "                except: continue\n",
    "\n",
    "    fig.savefig(f'../plots/group_differential_expression_{contrast_name}_{column}_{comparison}_{target}_{target_comparison}.pdf', format='pdf', transparent=True, backend='cairo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4CD Linkage Cluster Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for contrast_name, _, column, _, target, target_comparison in contrast_groupings:\n",
    "    for tar in (target, target_comparison):\n",
    "        # Get subject ids\n",
    "        group = None  # contrast_group\n",
    "        if group is None:\n",
    "            # Population\n",
    "            contrast_subjects = sum([v for k, v in get_contrast(contrast_name).items()], [])\n",
    "        else:\n",
    "            # Group\n",
    "            contrast_subjects = get_contrast(contrast_name)[group]\n",
    "\n",
    "        # Modify stack to include only contrast\n",
    "        df = np.nan_to_num(attention_stack[:, np.argwhere(np.array(columns)==column)[0][0], [s in contrast_subjects for s in subject_ids]])\n",
    "        new_subject_ids = [s for s in subject_ids if s in contrast_subjects]\n",
    "        df = pd.DataFrame(df, index=all_edges, columns=new_subject_ids)\n",
    "\n",
    "        # Get 100 most variant edges\n",
    "        df = df.iloc[df.to_numpy().var(axis=1).argsort()[::-1][:100]]\n",
    "\n",
    "        # Cluster\n",
    "        labels = KMeans(n_clusters=10, n_init=10).fit_predict(df.to_numpy().T)\n",
    "        labels += 1\n",
    "\n",
    "        # Get phenotypes\n",
    "        pheno = [meta.iloc[np.argwhere(meta['SubID'] == sid)[0][0]][tar] for sid in new_subject_ids]\n",
    "\n",
    "        # Format results\n",
    "        df = pd.DataFrame({'Cluster': labels, tar: pheno}, index=new_subject_ids)\n",
    "        df['count'] = 1\n",
    "        df = df.pivot_table(index='Cluster', columns=tar, values='count', aggfunc='sum').fillna(0)\n",
    "\n",
    "        # Transform to hypergeometric\n",
    "        df_np = df.to_numpy()\n",
    "        df_np_new = np.zeros_like(df_np)\n",
    "        for i, j in product(*[range(k) for k in df.shape]):\n",
    "            # i - cluster, j - target\n",
    "            dist = hypergeom(df_np.sum(), df_np[:, j].sum(), df_np[i, :].sum())\n",
    "            # Calculate probability of overrepresentation\n",
    "            df_np_new[i, j] = 1 - dist.cdf(df_np[i, j])\n",
    "        with np.errstate(divide='ignore'):\n",
    "            df_np_new = -np.log10(df_np_new)\n",
    "            df_np_new[np.isinf(df_np_new)] = np.nan\n",
    "        df = pd.DataFrame(df_np_new, index=df.index, columns=df.columns)\n",
    "\n",
    "        # Plot\n",
    "        fig, axs = get_mosaic([list(range(1))], scale=9)\n",
    "        sns.heatmap(df, vmin=0, vmax=3, cmap='rocket_r', cbar_kws={'label': '-log10(p)'}, ax=axs[0])\n",
    "        # plt.tight_layout()\n",
    "        fig.savefig(f'../plots/group_linkage_cluster_{contrast_name}_{column}_{tar}.pdf', format='pdf', transparent=True, backend='cairo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4E Aggregate Graph Enrichment (MANUAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No threshold provided, using threshold of 0.05016567523128692.\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 299268/299268 [00:00<00:00, 340656.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered from 4693 vertices and 50603 edges to 1565 vertices and 5755 edges via common edge filtering.\n",
      "No threshold provided, using threshold of 0.05386086725079708.\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 253981/253981 [00:00<00:00, 339890.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered from 4704 vertices and 49517 edges to 1365 vertices and 4839 edges via common edge filtering.\n",
      "No threshold provided, using threshold of 0.05016567523128692.\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 299268/299268 [00:00<00:00, 345498.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered from 4693 vertices and 50603 edges to 1565 vertices and 5755 edges via common edge filtering.\n",
      "No threshold provided, using threshold of 0.05386086725079708.\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 253981/253981 [00:00<00:00, 342338.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered from 4704 vertices and 49517 edges to 1365 vertices and 4839 edges via common edge filtering.\n",
      "No threshold provided, using threshold of 0.05016567523128692.\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 299268/299268 [00:00<00:00, 327908.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered from 4693 vertices and 50603 edges to 1565 vertices and 5755 edges via common edge filtering.\n",
      "No threshold provided, using threshold of 0.05386086725079708.\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 253981/253981 [00:00<00:00, 340098.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered from 4704 vertices and 49517 edges to 1365 vertices and 4839 edges via common edge filtering.\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Only top 100 are taken for aggregate due to memory concerns\n",
    "for contrast_name, group, column, _, _, _ in contrast_groupings:\n",
    "      # Load contrast\n",
    "      np.random.seed(42)\n",
    "      contrast_subjects = get_contrast(contrast_name)\n",
    "      gs = {\n",
    "            gname: concatenate_graphs(*[\n",
    "                  compute_graph(g)\n",
    "                  for g in load_many_graphs(np.random.choice(sids, 100, replace=False))[0]\n",
    "            ])\n",
    "            for gname, sids in contrast_subjects.items()\n",
    "      }\n",
    "\n",
    "      # Split into groups\n",
    "      # TODO: Make more general, perhaps add comparison group to arguments\n",
    "      g1_name = group\n",
    "      g1 = gs[g1_name]\n",
    "      g2_name = 'Control'\n",
    "      g2 = gs[g2_name]\n",
    "\n",
    "      # Get unique TFs\n",
    "      df = compare_graphs_enrichment(\n",
    "            g1,\n",
    "            g2,\n",
    "            sid_1=g1_name,\n",
    "            sid_2=g2_name,\n",
    "            nodes=synthetic_nodes_of_interest,\n",
    "            threshold=.01)\n",
    "\n",
    "      # Save to file\n",
    "      df.to_csv(f'../plots/genes_{contrast_name}_{group}_{column}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrichment\n",
    "for contrast_name, group, column, _, _, _ in contrast_groupings:\n",
    "    # MANUAL PROCESSING\n",
    "    # Run the output from above on Metascape as multiple gene list and perform\n",
    "    # enrichment.  From the all-in-one ZIP file, save the file from\n",
    "    # Enrichment_QC/GO_DisGeNET as '../plot/disgenet_{subject_id_1}_{subject_id_2}_{column}.csv' and\n",
    "    # Overlap_circos/CircosOverlapByGene.svg as '../plot/overlap_{subject_id_1}_{subject_id_2}_{column}.svg'\n",
    "\n",
    "    # Get enrichment\n",
    "    enrichment_file = f'../plots/disgenet_{contrast_name}_{group}_{column}.csv'\n",
    "    if enrichment_file is None: continue\n",
    "    enrichment = pd.read_csv(enrichment_file)\n",
    "\n",
    "    # Format\n",
    "    enrichment = format_enrichment(enrichment)\n",
    "\n",
    "    # Plot\n",
    "    fig, axs = get_mosaic([[0]*2], scale=9)\n",
    "    pl = sns.scatterplot(\n",
    "        enrichment,\n",
    "        x='Gene Set', y='Description',\n",
    "        size='-log10(p)',\n",
    "        color='black',\n",
    "        ax=axs[0])\n",
    "    # Formatting\n",
    "    pl.grid()\n",
    "    plt.xticks(rotation=90)\n",
    "    pl.set_aspect('equal', 'box')\n",
    "    pl.legend(title='-log10(p)', bbox_to_anchor=(1.2, 1.05))\n",
    "    # Zoom X1\n",
    "    margin = .5\n",
    "    min_xlim, max_xlim = pl.get_xlim()\n",
    "    min_xlim -= margin; max_xlim += margin\n",
    "    pl.set(xlim=(min_xlim, max_xlim))\n",
    "    # Save\n",
    "    fig.savefig(f'../plots/group_enrichment_{contrast_name}_{group}_{column}.pdf', format='pdf', transparent=True, backend='cairo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4X Variance Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get plots for each column\n",
    "# for contrast_name, _, column, comparison, _, _ in contrast_groupings:\n",
    "#     for col in (column, comparison):\n",
    "#         print(' - '.join((contrast_name, col)))\n",
    "\n",
    "#         # Get contrast\n",
    "#         contrast = get_contrast(contrast_name)\n",
    "\n",
    "#         # Compute\n",
    "#         df_subgroup = compute_contrast_summary(contrast, column=col)\n",
    "\n",
    "#         # Plot mean-sorted\n",
    "#         fig, axs = get_mosaic([list(range(1))], scale=9)\n",
    "#         plot_subgroup_heatmap(df_subgroup, ax=axs[0])\n",
    "#         plt.tight_layout()\n",
    "#         fig.savefig(f'../plots/group_variance_heatmap_{contrast_name}_{col}.pdf', format='pdf', transparent=True, backend='cairo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4X Cross-Validation Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Make all y-labels horizontal\n",
    "# for contrast_name, _, column, _, target, target_comparison in contrast_groupings:\n",
    "#     for tar in (target, target_comparison):\n",
    "#         # if contrast_name != 'c71x': continue\n",
    "#         print(' - '.join((contrast_name, column, tar)))\n",
    "#         # Get contrast\n",
    "#         contrast = get_contrast(contrast_name)\n",
    "\n",
    "#         # Compute prioritized edges\n",
    "#         # Get 100 most variant edges\n",
    "#         # TODO: Revise this method, maybe also consider means\n",
    "#         sids = sum([sids for _, sids in contrast.items()], [])\n",
    "#         df_subgroup = compute_contrast_summary(contrast, column=column)\n",
    "#         df = join_df_subgroup(df_subgroup, num_sort=100)\n",
    "#         prioritized_edges = list(df.index)\n",
    "\n",
    "#         # Plot\n",
    "#         # TODO: Maybe return to row-normalization\n",
    "#         fig, axs = get_mosaic([[0]], scale=9)\n",
    "#         df, acc = plot_prediction_confusion(contrast, meta=meta, column=column, target=tar, prioritized_edges=prioritized_edges, classifier_type='SGD', ax=axs[0])\n",
    "\n",
    "#         # Save plot\n",
    "#         fname_prefix = f'../plots/group_prioritized_edge_prediction_{contrast_name}_{column}_{tar}'\n",
    "#         fig.savefig(f'{fname_prefix}.pdf', format='pdf', transparent=True, backend='cairo')\n",
    "\n",
    "#         # Save text\n",
    "#         f_edges = open(f'{fname_prefix}.edges.txt', 'w')\n",
    "#         f_tfs = open(f'{fname_prefix}.tfs.txt', 'w')\n",
    "#         f_tgs = open(f'{fname_prefix}.tgs.txt', 'w')\n",
    "#         for edge in prioritized_edges:\n",
    "#             f_edges.write(edge + '\\n')\n",
    "#             tf, tg = edge.split(get_edge_string(['', '']))\n",
    "#             f_tfs.write(tf + '\\n')\n",
    "#             f_tgs.write(tg + '\\n')\n",
    "#         f_edges.close()\n",
    "#         f_tfs.close()\n",
    "#         f_tgs.close()\n",
    "\n",
    "#         # CLI\n",
    "#         print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot legend\n",
    "plt.clf()\n",
    "plot_legend(horizontal=False)\n",
    "plt.gca().axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../plots/graph_legend.pdf', format='pdf', transparent=True, backend='cairo')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
