{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T06:40:26.417400Z",
     "iopub.status.busy": "2023-07-13T06:40:26.417242Z",
     "iopub.status.idle": "2023-07-13T06:40:26.428582Z",
     "shell.execute_reply": "2023-07-13T06:40:26.427990Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T06:40:26.430818Z",
     "iopub.status.busy": "2023-07-13T06:40:26.430522Z",
     "iopub.status.idle": "2023-07-13T06:40:27.436429Z",
     "shell.execute_reply": "2023-07-13T06:40:27.435960Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import os\n",
    "\n",
    "import graph_tool.all as gt\n",
    "import matplotlib\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import hypergeom, pearsonr\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from functions import *\n",
    "\n",
    "\n",
    "# Graph-Tool compatibility\n",
    "plt.switch_backend('cairo')\n",
    "# Style\n",
    "sns.set_theme(context='talk', style='white', palette='Set2')\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/nck/repos/GNN-Plus/Attention Analysis/scripts/functions/file.py:122: DtypeWarning: Columns (5,157,158,159,161,164,165,166,167,170,172,176,177,178,179,189,191,193,195,276,285,286,287,288,289,290,291,292,293,294,295,297,342,345,346,462,465,566,570,571,572,573,574,580,582,583,584,586,589,592,594,597,599,601,603,606,607,611,613,615,617,618,620,622,625,684,686) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(META)\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "meta = get_meta()\n",
    "\n",
    "# Subject preview\n",
    "filtered = []\n",
    "for i, row in meta.iterrows():\n",
    "    try:\n",
    "        load_graph_by_id(row['SubID'])\n",
    "        assert not np.isnan(row['nps_MoodDysCurValue'])  # Has NPS information available\n",
    "        assert row['BRAAK_AD'] in (6,) and row['CERAD'] in (4,) and row['CDRScore'] in (3,)\n",
    "    except:\n",
    "        continue\n",
    "    filtered.append(f'{row[\"SubID\"]} {row[\"Ethnicity\"]} {row[\"Sex\"]}, {row[\"Age\"]}, BRAAK {row[\"BRAAK_AD\"]}, CERAD {row[\"CERAD\"]}, CDR {row[\"CDRScore\"]}, {row[\"Dx\"]}')\n",
    "filtered = np.sort(filtered)\n",
    "for i in range(len(filtered)):\n",
    "    # print(filtered[i])\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './attentions.pkl'\n",
    "if os.path.isfile(fname):\n",
    "    # Load data\n",
    "    with open('./attentions.pkl', 'rb') as f:\n",
    "        all_data = pickle.load(f)\n",
    "    attention_stack, all_edges, columns, subject_ids = all_data['data'], all_data['edges'], all_data['heads'], all_data['subject_ids']\n",
    "\n",
    "else:\n",
    "    # Parameters\n",
    "    # Scaled probably shouldn't be used, but better for visualization\n",
    "    # until results are more even\n",
    "    columns = get_attention_columns(scaled=False)\n",
    "    subject_ids = meta['SubID'].to_numpy()\n",
    "\n",
    "    # Load graphs\n",
    "    graphs, subject_ids = load_many_graphs(subject_ids, column=columns)\n",
    "    # graphs = [compute_graph(g) for g in graphs]\n",
    "\n",
    "    # # Get attentions\n",
    "    # df = {}\n",
    "    # for column in get_attention_columns():\n",
    "    #     attention, _ = compute_edge_summary(graphs, subject_ids=subject_ids)\n",
    "    #     attention = attention.set_index('Edge')\n",
    "    #     df[column] = attention.var(axis=1)\n",
    "\n",
    "\n",
    "    # Set indices to edges and clean\n",
    "    print('Fixing indices...')\n",
    "    for i in tqdm(range(len(graphs))):\n",
    "        graphs[i].index = graphs[i].apply(lambda r: get_edge_string([r['TF'], r['TG']]), axis=1)\n",
    "        graphs[i] = graphs[i].drop(columns=['TF', 'TG'])\n",
    "        # Remove duplicates\n",
    "        graphs[i] = graphs[i][~graphs[i].index.duplicated(keep='first')]\n",
    "\n",
    "    # Get all unique edges\n",
    "    print('Getting unique edges...')\n",
    "    all_edges = np.unique(sum([list(g.index) for g in graphs], []))\n",
    "\n",
    "\n",
    "    # Standardize index order\n",
    "    print('Standardizing indices...')\n",
    "    for i in tqdm(range(len(graphs))):\n",
    "        # Add missing indices and order based on `all_edges`\n",
    "        # to_add = [edge for edge in all_edges if edge not in list(graphs[i].index)]  # SLOW\n",
    "        to_add = list(set(all_edges) - set(graphs[i].index))\n",
    "\n",
    "        # Empty rows\n",
    "        new_rows = pd.DataFrame(\n",
    "            [[np.nan]*len(graphs[i].columns)]*len(to_add),\n",
    "            columns=graphs[i].columns,\n",
    "        ).set_index(pd.Series(to_add))\n",
    "        # Native concat\n",
    "        graphs[i] = pd.concat([graphs[i], new_rows]).loc[all_edges]\n",
    "\n",
    "    # Convert to numpy\n",
    "    graphs = [g.to_numpy() for g in graphs]\n",
    "    attention_stack = np.stack(graphs, axis=-1)\n",
    "    # attention_stack.shape = (Edge, Head, Subject)\n",
    "    # attention_stack.shape = (all_edges, columns, subject_ids)\n",
    "\n",
    "    # Save all data\n",
    "    all_data = {'data': attention_stack, 'edges': all_edges, 'heads': columns, 'subject_ids': subject_ids}\n",
    "    # np.savez('attentions.npz', **all_data)\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(\n",
    "            all_data,\n",
    "            f,\n",
    "            protocol=pickle.HIGHEST_PROTOCOL,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional useful parameters\n",
    "self_loops = [split_edge_string(s)[0] == split_edge_string(s)[1] for s in all_edges]\n",
    "self_loops = np.array(self_loops)\n",
    "# Remove self loops\n",
    "all_edges = all_edges[~self_loops]\n",
    "attention_stack = attention_stack[~self_loops]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available attention columns: ['att_D_AD_1', 'att_D_AD_2', 'att_D_SCZ_1', 'att_D_SCZ_2', 'att_D_no_prior_0', 'att_D_no_prior_1', 'att_D_no_prior_2', 'att_D_no_prior_3']\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "print(f'\\nAvailable attention columns: {get_attention_columns()}')\n",
    "column_ad = get_attention_columns()[0]\n",
    "column_scz = get_attention_columns()[2]\n",
    "column_data = get_attention_columns()[4]\n",
    "synthetic_nodes_of_interest = ['OPC', 'Micro', 'Oligo']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intra-Contrast Comparisons (Figure 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ..AAAAAABBBBCCCCDDDD\n",
      "    ..AAAAAABBBBCCCCDDDD\n",
      "    ..AAAAAABBBBCCCCDDDD\n",
      "    ..AAAAAABBBBCCCCDDDD\n",
      "    ..AAAAAABBBBCCCCDDDD\n",
      "    ..AAAAAAEEEEEEFFFFFF\n",
      "    ..AAAAAAEEEEEEFFFFFF\n",
      "    ..AAAAAAEEEEEEFFFFFF\n",
      "    ..AAAAAAEEEEEEFFFFFF\n",
      "    ..GGGGGGHHHHIIIIJJJJ\n",
      "    ..GGGGGGHHHHIIIIJJJJ\n",
      "    ..GGGGGGHHHHIIIIJJJJ\n",
      "    ..GGGGGGKKKKKKKKKKKK\n",
      "    ..GGGGGGKKKKKKKKKKKK\n",
      "    ..GGGGGGKKKKKKKKKKKK\n",
      "    ..GGGGGGKKKKKKKKKKKK\n",
      "    ..GGGGGGKKKKKKKKKKKK\n",
      "    LLLLLMMMMMNNNNNNNNNN\n",
      "    LLLLLMMMMMNNNNNNNNNN\n",
      "    LLLLLMMMMMNNNNNNNNNN\n",
      "    LLLLLMMMMMNNNNNNNNNN\n",
      "    ..........NNNNNNNNNN\n",
      "    ..........NNNNNNNNNN\n",
      "    ..........NNNNNNNNNN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Figure parameters\n",
    "param = {\n",
    "    'subjects': ['M35115', 'M35594'],\n",
    "    'columns': [column_data, column_ad, column_scz],\n",
    "    'column_names': ['Data Prioritization', 'AD Prioritization', 'SCZ Prioritization'],\n",
    "    'contrast': 'c15x',\n",
    "}\n",
    "\n",
    "# Generate palette\n",
    "palette = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "param['palette'] = {sid: rgba_to_hex(palette[i]) for i, sid in enumerate(param['subjects'])}\n",
    "\n",
    "# Create figure\n",
    "shape = \"\"\"\n",
    "    ..AAAAAABBBBIIIIMMMM\n",
    "    ..AAAAAABBBBIIIIMMMM\n",
    "    ..AAAAAABBBBIIIIMMMM\n",
    "    ..AAAAAABBBBIIIIMMMM\n",
    "    ..AAAAAABBBBIIIIMMMM\n",
    "    ..AAAAAACCCCCCLLLLLL\n",
    "    ..AAAAAACCCCCCLLLLLL\n",
    "    ..AAAAAACCCCCCLLLLLL\n",
    "    ..AAAAAACCCCCCLLLLLL\n",
    "    ..DDDDDDEEEEFFFFGGGG\n",
    "    ..DDDDDDEEEEFFFFGGGG\n",
    "    ..DDDDDDEEEEFFFFGGGG\n",
    "    ..DDDDDDHHHHHHHHHHHH\n",
    "    ..DDDDDDHHHHHHHHHHHH\n",
    "    ..DDDDDDHHHHHHHHHHHH\n",
    "    ..DDDDDDHHHHHHHHHHHH\n",
    "    ..DDDDDDHHHHHHHHHHHH\n",
    "    NNNNNJJJJJKKKKKKKKKK\n",
    "    NNNNNJJJJJKKKKKKKKKK\n",
    "    NNNNNJJJJJKKKKKKKKKK\n",
    "    NNNNNJJJJJKKKKKKKKKK\n",
    "    ..........KKKKKKKKKK\n",
    "    ..........KKKKKKKKKK\n",
    "    ..........KKKKKKKKKK\n",
    "\"\"\"\n",
    "shape = alphabetize_shape(shape)\n",
    "shape_array = [s.replace(' ', '') for s in shape.split('\\n')[1:-1]]\n",
    "shape_array = np.array([list(s) for s in shape_array])\n",
    "print(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Panels (A, G)\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 378/378 [00:00<00:00, 225429.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating positions...\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 171/171 [00:00<00:00, 164388.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 207/207 [00:00<00:00, 178756.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cell Type Graph (B, D)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating positions...\n",
      "\n",
      "Cell Type Priority (C)\n",
      "\n",
      "Attention Histogram (E, F)\n",
      "\n",
      "Edge Comparisons (H, I, J)\n",
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 293091.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 276674.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing duplicate edges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 8755/8755 [00:00<00:00, 263865.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Module Analysis (K)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thema/miniconda3/envs/GNN/lib/python3.11/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outcome Icon (L)\n",
      "\n",
      "Edge Discovery (M)\n",
      "\n",
      "Enrichment (N)\n",
      "\n",
      "Saving Figure...\n"
     ]
    }
   ],
   "source": [
    "# Subplot layout (doesn't work well with constrained layout)\n",
    "# NOTE: This cannot be used, as constrained layout has glitches\n",
    "# (see https://github.com/matplotlib/matplotlib/issues/23290)\n",
    "# with uneven mosaics\n",
    "# fig, axs = get_mosaic(shape, figsize=(int((3/2) * shape_array.shape[1]), int((3/2) * shape_array.shape[0])), constrained_layout=False)\n",
    "\n",
    "# Subfigure layout (longer)\n",
    "# NOTE: Constrained layout will fail for all\n",
    "# subplots if a single one is not able to scale.\n",
    "# Also, sometimes leaving a subfigure blank will\n",
    "# cause it to fail, especially if on an edge.\n",
    "# It is VERY finnicky.\n",
    "# SOLUTION: Save again using `fig.savefig(...)`\n",
    "# and it will run without warning.  Then, you\n",
    "# can visually inspect for scaling issues.\n",
    "fig, axs = create_subfigure_mosaic(shape_array)\n",
    "fig.set_constrained_layout_pads(w_pad=0, h_pad=0, wspace=.4, hspace=.4)\n",
    "\n",
    "\n",
    "# Plot all panels\n",
    "\n",
    "axs_lab = ['A', 'G']\n",
    "print(f'Individual Panels ({\", \".join(axs_lab)})')\n",
    "plot_graph_comparison_from_sids(param['subjects'], axs={0: axs[axs_lab[0]], 1: axs[axs_lab[1]]}, column=param['columns'][0], vertex_ids=synthetic_nodes_of_interest)\n",
    "# Legend\n",
    "plot_legend(horizontal=False, loc='center', bbox_to_anchor=(.5, -.1), ax=axs[axs_lab[0]])\n",
    "\n",
    "\n",
    "axs_lab = ['B', 'D']\n",
    "print(f'\\nCell Type Graph ({\", \".join(axs_lab)})')\n",
    "pos_dict = {}\n",
    "for ax, column, title in zip([axs[lab] for lab in axs_lab], param['columns'], param['column_names']):\n",
    "    pos_dict = plot_ct_graph_from_sid(param['subjects'][0], ax=ax, column=column, **pos_dict)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "axs_lab = ['C']\n",
    "print(f'\\nCell Type Priority ({\", \".join(axs_lab)})')\n",
    "plot_ct_individual_edges(param['subjects'][0], **all_data, column=param['columns'][0], ax=axs[axs_lab[0]])\n",
    "axs[axs_lab[0]].set_title(param['column_names'][0])\n",
    "axs[axs_lab[0]].set_xlabel(None)\n",
    "\n",
    "\n",
    "axs_lab = ['E', 'F']\n",
    "print(f'\\nAttention Histogram ({\", \".join(axs_lab)})')\n",
    "ax_ymax = 1\n",
    "for ax, column, title in zip([axs[lab] for lab in axs_lab], param['columns'], param['column_names']):\n",
    "    plot_attention_histogram(param['subjects'][0], **all_data, column=column, ax=ax)\n",
    "    ax.set_xlabel(title)\n",
    "    # Record limits\n",
    "    ylim = ax.get_ylim()\n",
    "    if ylim[1] > ax_ymax: ax_ymax = ylim[1]\n",
    "# Set axis limits\n",
    "for ax in [axs[lab] for lab in axs_lab]:\n",
    "    ax.set_ylim(1, ax_ymax)\n",
    "\n",
    "\n",
    "axs_lab = ['H', 'I', 'J']\n",
    "print(f'\\nEdge Comparisons ({\", \".join(axs_lab)})')\n",
    "for ax, column, title in zip([axs[lab] for lab in axs_lab], param['columns'], param['column_names']):\n",
    "    ax.clear()\n",
    "    plot_edge_comparison_from_sids(param['subjects'], ax=ax, column=column, palette=param['palette'], highlight_outliers=True)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "axs_lab = ['K']\n",
    "print(f'\\nModule Analysis ({\", \".join(axs_lab)})')\n",
    "plot_module_scores_from_sids(param['subjects'], ax=axs[axs_lab[0]], palette=param['palette'], column=param['columns'][0])\n",
    "axs[axs_lab[0]].set_title(param['column_names'][0])\n",
    "\n",
    "\n",
    "# NOT USING\n",
    "# axs_lab = ['K']\n",
    "# print(f'\\nCell Type Linkage Analysis ({\", \".join(axs_lab)})')\n",
    "# plot_ct_edges(param['contrast'], **all_data, column=param['columns'][0], ax=axs[axs_lab[0]])\n",
    "# axs[axs_lab[0]].set_title(param['column_names'][0])\n",
    "\n",
    "\n",
    "axs_lab = ['L']\n",
    "print(f'\\nOutcome Icon ({\", \".join(axs_lab)})')\n",
    "# Manual, so no visualization is needed\n",
    "axs[axs_lab[0]].axis('off')\n",
    "\n",
    "\n",
    "axs_lab = ['M']\n",
    "print(f'\\nEdge Discovery ({\", \".join(axs_lab)})')\n",
    "plot_edge_discovery(**all_data, column=param['columns'][0], ax=axs[axs_lab[0]])\n",
    "axs[axs_lab[0]].set_title(param['column_names'][0])\n",
    "\n",
    "\n",
    "axs_lab = ['N']\n",
    "print(f'\\nEnrichment ({\", \".join(axs_lab)})')\n",
    "# Compute gene list\n",
    "fname = f'../plots/genes_{\"_\".join(param[\"subjects\"])}.csv'\n",
    "if not os.path.isfile(fname):\n",
    "    df = compute_all_important_genes_from_sids(param['subjects'], **all_data, vertex_ids=synthetic_nodes_of_interest, columns=param['columns'], column_names=param['column_names'])\n",
    "    df.to_csv(fname, index=False)\n",
    "# MANUAL PROCESSING\n",
    "# Run the output from above on Metascape as multiple gene list and perform\n",
    "# enrichment.  From the all-in-one ZIP file, save the file from\n",
    "# Enrichment_GO/GO_membership.csv as `fname` below\n",
    "# Read enrichment\n",
    "fname = f'../plots/go_{\"_\".join(param[\"subjects\"])}.csv'\n",
    "if os.path.isfile(fname):\n",
    "    plot_enrichment_from_fname(fname, ax=axs[axs_lab[0]])\n",
    "axs[axs_lab[0]].set_xlabel(None)\n",
    "\n",
    "\n",
    "# Save figure\n",
    "# NOTE: `bbox_inches='tight'` will eliminate whitespace from edges, even from '.' in mosaic.\n",
    "# However, this should not affect scaling\n",
    "print('\\nSaving Figure...')\n",
    "fig.savefig(f'../plots/figure_3.pdf', format='pdf', bbox_inches='tight', transparent=True, backend='cairo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make label overlay\n",
    "# NOTE: Cannot be done on main plot, as constrained layout does not support it\n",
    "fig, axs = create_subfigure_mosaic(shape_array, layout=None)\n",
    "fig.set_constrained_layout_pads(w_pad=0, h_pad=0, wspace=0, hspace=0)\n",
    "for label, ax in axs.items():\n",
    "    # Clear axis\n",
    "    ax.axis('off')\n",
    "    # Figure level\n",
    "    pos = ax.figure.transSubfigure.transform((0, 1)) + 20 * np.array([1, -1])\n",
    "    pos = fig.transFigure.inverted().transform(pos)\n",
    "    plt.text(*pos, label, ha='right', va='bottom', fontweight='bold', fontsize='xx-large', transform=fig.transFigure)\n",
    "    # Subfigure level\n",
    "    # pos = ax.figure.transSubfigure.inverted().transform(ax.figure.transSubfigure.transform((0, 1)) + 20 * np.array([1, -1]))\n",
    "    # plt.text(*pos, label, ha='right', va='bottom', fontweight='bold', fontsize='xx-large', transform=ax.figure.transSubfigure)\n",
    "fig.savefig(f'../plots/figure_3_labels.pdf', format='pdf', bbox_inches='tight', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1503800/1503800 [00:29<00:00, 51481.03it/s]\n"
     ]
    }
   ],
   "source": [
    "def compute_prs_difference(meta, *, data, edges, heads, subject_ids):\n",
    "    import scipy.stats\n",
    "\n",
    "    # Get PRS scores\n",
    "    # NOTE: There are several prs_SCZ columns, which to use is unclear\n",
    "    prs = meta.set_index('SubID').loc[subject_ids, 'prs_SCZ.3.5_MVP']\n",
    "\n",
    "    # Filter to subjects with PRS scores\n",
    "    present_id_mask = (~prs.isna()).to_numpy()\n",
    "    data = data[:, :, present_id_mask]\n",
    "    subject_ids = np.array(subject_ids)[present_id_mask]\n",
    "    prs = prs.loc[~prs.isna()].to_numpy()\n",
    "\n",
    "    # Compute correlations\n",
    "    df = pd.DataFrame(columns=['edge', 'head', 'n', 'correlation', 'p'])\n",
    "    np.random.seed(42)\n",
    "    for e, h in tqdm(product(range(len(edges)), range(len(heads))), total=len(edges)*len(heads)):\n",
    "        # Random sample (for testing)\n",
    "        if np.random.rand() < .95: continue\n",
    "\n",
    "        # Acquire and filter to present data\n",
    "        data_eh = data[e, h]\n",
    "        present_data_mask = ~np.isnan(data_eh)\n",
    "\n",
    "        # Skip if not enough data\n",
    "        if present_data_mask.sum() < 2 or data_eh[present_data_mask].std() == 0 or prs[present_data_mask].std() == 0: continue\n",
    "\n",
    "        # Compute correlation\n",
    "        # TODO: Compute using matrix\n",
    "        corr, p = scipy.stats.pearsonr(data_eh[present_data_mask], prs[present_data_mask])\n",
    "\n",
    "        # Record\n",
    "        df.loc[df.shape[0]] = [edges[e], heads[h], present_data_mask.sum(), corr, p]\n",
    "\n",
    "    # Add FDR\n",
    "    # QUESTION: Does FDR need to be adjusted for varying population size?\n",
    "    df['fdr'] = scipy.stats.false_discovery_control(df['p'].to_numpy())\n",
    "\n",
    "    # Sort by p/fdr\n",
    "    df = df.sort_values('p').reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = compute_prs_difference(meta, **all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edge</th>\n",
       "      <th>head</th>\n",
       "      <th>n</th>\n",
       "      <th>correlation</th>\n",
       "      <th>p</th>\n",
       "      <th>fdr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOXD4L5 -- EN_L3_5_IT_1</td>\n",
       "      <td>att_D_SCZ_2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.058845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POU2F2 -- SLC8A1</td>\n",
       "      <td>att_D_SCZ_2</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.834572</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.704172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CERS6 -- IN_PVALB</td>\n",
       "      <td>att_D_no_prior_0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.997797</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.704172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCF7L2 -- ALOX12P2</td>\n",
       "      <td>att_D_no_prior_3</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.903109</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.704172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LEF1 -- SMC</td>\n",
       "      <td>att_D_no_prior_1</td>\n",
       "      <td>34</td>\n",
       "      <td>-0.597867</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.704172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RXRG -- EN_L3_5_IT_1</td>\n",
       "      <td>att_D_no_prior_2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.871185</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.704172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ETS1 -- SLC9A3R2</td>\n",
       "      <td>att_D_SCZ_1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.704172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NFATC1 -- EN_L6_IT_1</td>\n",
       "      <td>att_D_AD_1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.879696</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.926489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TCF7L1 -- MASP1</td>\n",
       "      <td>att_D_no_prior_2</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.842374</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BACH2 -- PCDH11Y</td>\n",
       "      <td>att_D_no_prior_2</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.974790</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PPARD -- NABP1</td>\n",
       "      <td>att_D_no_prior_2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999048</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OLIG2 -- Oligo</td>\n",
       "      <td>att_D_no_prior_0</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.800681</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PRDM6 -- UACA</td>\n",
       "      <td>att_D_no_prior_2</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.972950</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TEAD1 -- IGFBP2</td>\n",
       "      <td>att_D_AD_2</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.999998</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HOXB3 -- IN_PVALB_CHC</td>\n",
       "      <td>att_D_AD_1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.946125</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CEBPA -- CD14</td>\n",
       "      <td>att_D_no_prior_2</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.998695</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>OLIG1 -- PLPPR1</td>\n",
       "      <td>att_D_no_prior_2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.998634</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LEF1 -- SLC6A13</td>\n",
       "      <td>att_D_no_prior_2</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.601619</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BACH1 -- MSR1</td>\n",
       "      <td>att_D_no_prior_3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.988455</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TAL1 -- TLE3</td>\n",
       "      <td>att_D_no_prior_1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.939872</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       edge              head   n  correlation         p  \\\n",
       "0   FOXD4L5 -- EN_L3_5_IT_1       att_D_SCZ_2   4     0.999997  0.000003   \n",
       "1          POU2F2 -- SLC8A1       att_D_SCZ_2  15    -0.834572  0.000109   \n",
       "2         CERS6 -- IN_PVALB  att_D_no_prior_0   5     0.997797  0.000124   \n",
       "3        TCF7L2 -- ALOX12P2  att_D_no_prior_3  11    -0.903109  0.000139   \n",
       "4               LEF1 -- SMC  att_D_no_prior_1  34    -0.597867  0.000188   \n",
       "5      RXRG -- EN_L3_5_IT_1  att_D_no_prior_2  12     0.871185  0.000224   \n",
       "6          ETS1 -- SLC9A3R2       att_D_SCZ_1   3    -1.000000  0.000237   \n",
       "7      NFATC1 -- EN_L6_IT_1        att_D_AD_1  11     0.879696  0.000357   \n",
       "8           TCF7L1 -- MASP1  att_D_no_prior_2  12    -0.842374  0.000584   \n",
       "9          BACH2 -- PCDH11Y  att_D_no_prior_2   6    -0.974790  0.000945   \n",
       "10           PPARD -- NABP1  att_D_no_prior_2   4     0.999048  0.000952   \n",
       "11           OLIG2 -- Oligo  att_D_no_prior_0  13    -0.800681  0.001007   \n",
       "12            PRDM6 -- UACA  att_D_no_prior_2   6    -0.972950  0.001088   \n",
       "13          TEAD1 -- IGFBP2        att_D_AD_2   3    -0.999998  0.001180   \n",
       "14    HOXB3 -- IN_PVALB_CHC        att_D_AD_1   7     0.946125  0.001257   \n",
       "15            CEBPA -- CD14  att_D_no_prior_2   4    -0.998695  0.001305   \n",
       "16          OLIG1 -- PLPPR1  att_D_no_prior_2   4     0.998634  0.001366   \n",
       "17          LEF1 -- SLC6A13  att_D_no_prior_2  25    -0.601619  0.001466   \n",
       "18            BACH1 -- MSR1  att_D_no_prior_3   5     0.988455  0.001486   \n",
       "19             TAL1 -- TLE3  att_D_no_prior_1   7     0.939872  0.001648   \n",
       "\n",
       "         fdr  \n",
       "0   0.058845  \n",
       "1   0.704172  \n",
       "2   0.704172  \n",
       "3   0.704172  \n",
       "4   0.704172  \n",
       "5   0.704172  \n",
       "6   0.704172  \n",
       "7   0.926489  \n",
       "8   1.000000  \n",
       "9   1.000000  \n",
       "10  1.000000  \n",
       "11  1.000000  \n",
       "12  1.000000  \n",
       "13  1.000000  \n",
       "14  1.000000  \n",
       "15  1.000000  \n",
       "16  1.000000  \n",
       "17  1.000000  \n",
       "18  1.000000  \n",
       "19  1.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter-Contrast Comparisons (Figure 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combinations\n",
    "# # TODO: Potentially move each entry to dictionary, so changes in order\n",
    "# #   are easier to propagate\n",
    "# contrast_groupings = [\n",
    "#     # (contrast name, contrast group, attention column, comparison column, target meta column, other target meta column)\n",
    "#     # for contrast_name, contrast_group, column, comparison, target, target_comparison in contrast_groupings:\n",
    "#     # TODO: Revise ethnicity prediction\n",
    "#     ('c15x', 'AD', column_ad, column_data, 'BRAAK_AD', 'Ethnicity'),\n",
    "#     ('c15x', 'AD', column_data, column_ad, 'BRAAK_AD', 'Ethnicity'),\n",
    "#     ('c15x', 'AD', column_data, column_ad, 'nps_MoodDysCurValue', 'nps_WtGainCurValue'),\n",
    "#     # ('c06x', 'AD', column_ad, column_data, 'BRAAK_AD', 'nps_MoodDysCurValue'),  # Eventually SCZ, BP and such\n",
    "#     # ('c71x', 'MoodDys', column_data, column_ad, 'nps_MoodDysCurValue'),  # Dysphoria\n",
    "#     # ('c72x', 'DecInt', column_data, column_ad, 'nps_DecIntCurValue'),  # Anhedonia\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4B Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for contrast_name, _, column, comparison, target, target_comparison in contrast_groupings:\n",
    "#     # Filter attention stack to contrast\n",
    "#     contrast = get_contrast(contrast_name)\n",
    "#     contrast_subject_ids = sum([contrast[group] for group in contrast], [])\n",
    "#     contrast_mask = [sid in contrast_subject_ids for sid in subject_ids]\n",
    "#     contrast_subject_ids = np.array(subject_ids)[contrast_mask]\n",
    "#     contrast_stack = attention_stack[:, :, contrast_mask]\n",
    "\n",
    "#     # Filter to 1000 most variant edges\n",
    "#     top_variant_edge_idx = np.nan_to_num(\n",
    "#         contrast_stack[:, np.argwhere(np.array(columns)==column)[0][0]]).var(axis=1).argsort()[::-1][:1000]\n",
    "#     contrast_stack = contrast_stack[top_variant_edge_idx]\n",
    "#     edge_names = all_edges[top_variant_edge_idx]\n",
    "\n",
    "#     # Correlation df\n",
    "#     df = pd.DataFrame(\n",
    "#         contrast_stack[:, np.argwhere(np.array(columns)==column)[0][0]],\n",
    "#         index=pd.Series(all_edges[top_variant_edge_idx]),\n",
    "#         columns=contrast_subject_ids).T\n",
    "#     df = df.join(meta.set_index('SubID')[[target, target_comparison]]).reset_index(drop=True)\n",
    "#     # Select edge which most cleanly separates `target`\n",
    "#     # top_distinct_edge_idx = df.drop(target_comparison, axis=1).groupby(target).mean().var(axis=0).argsort()[-1]\n",
    "#     # Select top 3 most correlating edges\n",
    "#     edge_name = df.drop(target_comparison, axis=1).corr()[target].abs().drop(target).sort_values(ascending=False)[:3].index.to_numpy()\n",
    "#     top_distinct_edge_idx = [np.argwhere(df.columns==edge)[0][0] for edge in edge_name]\n",
    "#     contrast_stack = contrast_stack[top_distinct_edge_idx]\n",
    "\n",
    "#     # Plot\n",
    "#     fig, axs = get_mosaic(np.array(sum([[i]*2 for i in range(6)], [])).reshape((2, -1)), scale=5)\n",
    "#     # axs[0].sharex(axs[1])\n",
    "#     sns.despine()\n",
    "\n",
    "#     for i in range(3):\n",
    "#         # Filter\n",
    "#         contrast_stack_i = contrast_stack[i]\n",
    "#         edge_name_i = edge_name[i]\n",
    "\n",
    "#         # Scale attention\n",
    "#         # TODO: Remove once heads are balanced\n",
    "#         contrast_stack_i = contrast_stack_i / np.nan_to_num(contrast_stack_i).max(axis=1).reshape((-1, 1))\n",
    "\n",
    "#         # Format\n",
    "#         df = pd.DataFrame(contrast_stack_i, index=pd.Series(columns), columns=contrast_subject_ids)\n",
    "#         df = df.reset_index(names='Head').melt(id_vars='Head', var_name='Subject', value_name=edge_name_i).dropna()  # Melt\n",
    "#         df = df.set_index('Subject').join(meta.set_index('SubID')[[target, target_comparison]]).reset_index()  # Join meta\n",
    "\n",
    "#         # Filter to target heads\n",
    "#         df = df.loc[df['Head'].apply(lambda s: s in (column, comparison))]\n",
    "\n",
    "#         # Main target\n",
    "#         p1 = sns.violinplot(data=df, x='Head', y=edge_name_i, hue=target, ax=axs[i])\n",
    "#         p1.legend(title=target, bbox_to_anchor=(1.1, 1.05))\n",
    "#         p1.set(xlabel=None, xticklabels=[])\n",
    "\n",
    "#         # Comparison target\n",
    "#         p2 = sns.violinplot(data=df, x='Head', y=edge_name_i, hue=target_comparison, ax=axs[i+3])\n",
    "#         p2.legend(title=target_comparison, bbox_to_anchor=(1.1, 1.05))\n",
    "#         plt.sca(p2)\n",
    "#         plt.xticks(rotation=60)\n",
    "\n",
    "#         # Get correlation p-values for targets (which must be numeric)\n",
    "#         for j, tar in enumerate((target, target_comparison)):\n",
    "#             for k, c in enumerate(np.unique(df['Head'])):\n",
    "#                 try:\n",
    "#                     pval = pearsonr(df.loc[df['Head']==c, edge_name_i], df.loc[df['Head']==c, tar])[1]\n",
    "#                     axs[i+3*j].text(k, axs[i+3*j].get_ylim()[0] - (.15 if not j else .3), f'p={pval:.1e}', ha='center', va='center')\n",
    "#                 except: continue\n",
    "\n",
    "#     fig.savefig(f'../plots/group_differential_expression_{contrast_name}_{column}_{comparison}_{target}_{target_comparison}.pdf', format='pdf', transparent=True, backend='cairo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4CD Linkage Cluster Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for contrast_name, _, column, _, target, target_comparison in contrast_groupings:\n",
    "#     for tar in (target, target_comparison):\n",
    "#         # Get subject ids\n",
    "#         group = None  # contrast_group\n",
    "#         if group is None:\n",
    "#             # Population\n",
    "#             contrast_subjects = sum([v for k, v in get_contrast(contrast_name).items()], [])\n",
    "#         else:\n",
    "#             # Group\n",
    "#             contrast_subjects = get_contrast(contrast_name)[group]\n",
    "\n",
    "#         # Modify stack to include only contrast\n",
    "#         df = np.nan_to_num(attention_stack[:, np.argwhere(np.array(columns)==column)[0][0], [s in contrast_subjects for s in subject_ids]])\n",
    "#         new_subject_ids = [s for s in subject_ids if s in contrast_subjects]\n",
    "#         df = pd.DataFrame(df, index=all_edges, columns=new_subject_ids)\n",
    "\n",
    "#         # Get 100 most variant edges\n",
    "#         df = df.iloc[df.to_numpy().var(axis=1).argsort()[::-1][:100]]\n",
    "\n",
    "#         # Cluster\n",
    "#         labels = KMeans(n_clusters=10, n_init=10).fit_predict(df.to_numpy().T)\n",
    "#         labels += 1\n",
    "\n",
    "#         # Get phenotypes\n",
    "#         pheno = [meta.iloc[np.argwhere(meta['SubID'] == sid)[0][0]][tar] for sid in new_subject_ids]\n",
    "\n",
    "#         # Format results\n",
    "#         df = pd.DataFrame({'Cluster': labels, tar: pheno}, index=new_subject_ids)\n",
    "#         df['count'] = 1\n",
    "#         df = df.pivot_table(index='Cluster', columns=tar, values='count', aggfunc='sum').fillna(0)\n",
    "\n",
    "#         # Transform to hypergeometric\n",
    "#         df_np = df.to_numpy()\n",
    "#         df_np_new = np.zeros_like(df_np)\n",
    "#         for i, j in product(*[range(k) for k in df.shape]):\n",
    "#             # i - cluster, j - target\n",
    "#             dist = hypergeom(df_np.sum(), df_np[:, j].sum(), df_np[i, :].sum())\n",
    "#             # Calculate probability of overrepresentation\n",
    "#             df_np_new[i, j] = 1 - dist.cdf(df_np[i, j])\n",
    "#         with np.errstate(divide='ignore'):\n",
    "#             df_np_new = -np.log10(df_np_new)\n",
    "#             df_np_new[np.isinf(df_np_new)] = np.nan\n",
    "#         df = pd.DataFrame(df_np_new, index=df.index, columns=df.columns)\n",
    "\n",
    "#         # Plot\n",
    "#         fig, axs = get_mosaic([list(range(1))], scale=9)\n",
    "#         sns.heatmap(df, vmin=0, vmax=3, cmap='rocket_r', cbar_kws={'label': '-log10(p)'}, ax=axs[0])\n",
    "#         # plt.tight_layout()\n",
    "#         fig.savefig(f'../plots/group_linkage_cluster_{contrast_name}_{column}_{tar}.pdf', format='pdf', transparent=True, backend='cairo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4E Aggregate Graph Enrichment (MANUAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: Only top 100 are taken for aggregate due to memory concerns\n",
    "# for contrast_name, group, column, _, _, _ in contrast_groupings:\n",
    "#       # Load contrast\n",
    "#       np.random.seed(42)\n",
    "#       contrast_subjects = get_contrast(contrast_name)\n",
    "#       gs = {\n",
    "#             gname: concatenate_graphs(*[\n",
    "#                   compute_graph(g)\n",
    "#                   for g in load_many_graphs(np.random.choice(sids, 100, replace=False))[0]\n",
    "#             ])\n",
    "#             for gname, sids in contrast_subjects.items()\n",
    "#       }\n",
    "\n",
    "#       # Split into groups\n",
    "#       # TODO: Make more general, perhaps add comparison group to arguments\n",
    "#       g1_name = group\n",
    "#       g1 = gs[g1_name]\n",
    "#       g2_name = 'Control'\n",
    "#       g2 = gs[g2_name]\n",
    "\n",
    "#       # Get unique TFs\n",
    "#       df = compare_graphs_enrichment(\n",
    "#             g1,\n",
    "#             g2,\n",
    "#             sid_1=g1_name,\n",
    "#             sid_2=g2_name,\n",
    "#             nodes=synthetic_nodes_of_interest,\n",
    "#             threshold=.01)\n",
    "\n",
    "#       # Save to file\n",
    "#       df.to_csv(f'../plots/genes_{contrast_name}_{group}_{column}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enrichment\n",
    "# for contrast_name, group, column, _, _, _ in contrast_groupings:\n",
    "#     # MANUAL PROCESSING\n",
    "#     # Run the output from above on Metascape as multiple gene list and perform\n",
    "#     # enrichment.  From the all-in-one ZIP file, save the file from\n",
    "#     # Enrichment_QC/GO_DisGeNET as '../plot/disgenet_{subject_id_1}_{subject_id_2}_{column}.csv' and\n",
    "#     # Overlap_circos/CircosOverlapByGene.svg as '../plot/overlap_{subject_id_1}_{subject_id_2}_{column}.svg'\n",
    "\n",
    "#     # Get enrichment\n",
    "#     enrichment_file = f'../plots/disgenet_{contrast_name}_{group}_{column}.csv'\n",
    "#     if enrichment_file is None: continue\n",
    "#     enrichment = pd.read_csv(enrichment_file)\n",
    "\n",
    "#     # Format\n",
    "#     enrichment = format_enrichment(enrichment)\n",
    "\n",
    "#     # Plot\n",
    "#     fig, axs = get_mosaic([[0]*2], scale=9)\n",
    "#     pl = sns.scatterplot(\n",
    "#         enrichment,\n",
    "#         x='Gene Set', y='Description',\n",
    "#         size='-log10(p)',\n",
    "#         color='black',\n",
    "#         ax=axs[0])\n",
    "#     # Formatting\n",
    "#     pl.grid()\n",
    "#     plt.xticks(rotation=90)\n",
    "#     pl.set_aspect('equal', 'box')\n",
    "#     pl.legend(title='-log10(p)', bbox_to_anchor=(1.2, 1.05))\n",
    "#     # Zoom X1\n",
    "#     margin = .5\n",
    "#     min_xlim, max_xlim = pl.get_xlim()\n",
    "#     min_xlim -= margin; max_xlim += margin\n",
    "#     pl.set(xlim=(min_xlim, max_xlim))\n",
    "#     # Save\n",
    "#     fig.savefig(f'../plots/group_enrichment_{contrast_name}_{group}_{column}.pdf', format='pdf', transparent=True, backend='cairo')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
