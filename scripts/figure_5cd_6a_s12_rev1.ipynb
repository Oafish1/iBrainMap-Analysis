{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T06:40:26.417400Z",
     "iopub.status.busy": "2023-07-13T06:40:26.417242Z",
     "iopub.status.idle": "2023-07-13T06:40:26.428582Z",
     "shell.execute_reply": "2023-07-13T06:40:26.427990Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T06:40:26.430818Z",
     "iopub.status.busy": "2023-07-13T06:40:26.430522Z",
     "iopub.status.idle": "2023-07-13T06:40:27.436429Z",
     "shell.execute_reply": "2023-07-13T06:40:27.435960Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import os\n",
    "\n",
    "import graph_tool.all as gt\n",
    "import matplotlib\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from scipy.stats import hypergeom, pearsonr\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from functions import *\n",
    "\n",
    "\n",
    "# Graph-Tool compatibility\n",
    "plt.switch_backend('cairo')\n",
    "\n",
    "# Style\n",
    "sns.set_theme(context='talk', style='white', palette='Set2')\n",
    "plt.rcParams.update({\n",
    "    'pdf.fonttype': 42,\n",
    "    'ps.fonttype': 42,\n",
    "    'font.size': 22,\n",
    "    'axes.titlesize': 'medium',\n",
    "    'axes.labelsize': 'large',\n",
    "    'xtick.labelsize': 'medium',\n",
    "    'ytick.labelsize': 'medium',\n",
    "    'legend.fontsize': 'medium',\n",
    "    'legend.title_fontsize': 'medium',\n",
    "    'figure.titlesize': 'x-large',\n",
    "})\n",
    "\n",
    "# Figure transparency\n",
    "# matplotlib.rcParams['figure.facecolor'] = (1., 0., 0., 0.3)  # Debugging\n",
    "matplotlib.rcParams['figure.facecolor'] = (1., 0., 0., 0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1022/1022 [00:15<00:00, 67.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Integrity check\n",
    "check_ct_edge_specificity()  # Check for duplicate edges with different attentions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "meta = get_meta()\n",
    "\n",
    "# Subject preview\n",
    "filtered = []\n",
    "for i, row in meta.iterrows():\n",
    "    try:\n",
    "        load_graph_by_id(row['SubID'])\n",
    "        assert not np.isnan(row['nps_MoodDysCurValue'])  # Has NPS information available\n",
    "        assert row['BRAAK_AD'] in (6,) and row['CERAD'] in (4,) and row['CDRScore'] in (3,)\n",
    "    except:\n",
    "        continue\n",
    "    filtered.append(f'{row[\"SubID\"]} {row[\"Ethnicity\"]} {row[\"Sex\"]}, {row[\"Age\"]}, BRAAK {row[\"BRAAK_AD\"]}, CERAD {row[\"CERAD\"]}, CDR {row[\"CDRScore\"]}, {row[\"Dx\"]}')\n",
    "filtered = np.sort(filtered)\n",
    "for i in range(len(filtered)):\n",
    "    # print(filtered[i])\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './attentions.pkl'\n",
    "if os.path.isfile(fname):\n",
    "    # Load data\n",
    "    with open('./attentions.pkl', 'rb') as f:\n",
    "        all_data = pickle.load(f)\n",
    "    attention_stack, all_edges, columns, subject_ids = all_data['data'], all_data['edges'], all_data['heads'], all_data['subject_ids']\n",
    "\n",
    "else:\n",
    "    # Parameters\n",
    "    # Scaled probably shouldn't be used, but better for visualization\n",
    "    # until results are more even\n",
    "    columns = get_attention_columns(scaled=False)\n",
    "    subject_ids = meta['SubID'].to_numpy()\n",
    "\n",
    "    # Load graphs\n",
    "    graphs, subject_ids = load_many_graphs(subject_ids, column=columns)\n",
    "    # graphs = [compute_graph(g) for g in graphs]\n",
    "\n",
    "    # # Get attentions\n",
    "    # df = {}\n",
    "    # for column in get_attention_columns():\n",
    "    #     attention, _ = compute_edge_summary(graphs, subject_ids=subject_ids)\n",
    "    #     attention = attention.set_index('Edge')\n",
    "    #     df[column] = attention.var(axis=1)\n",
    "\n",
    "\n",
    "    # Set indices to edges and clean\n",
    "    print('Fixing indices...')\n",
    "    for i in tqdm(range(len(graphs))):\n",
    "        graphs[i].index = graphs[i].apply(lambda r: get_edge_string([r['TF'], r['TG']]), axis=1)\n",
    "        graphs[i] = graphs[i].drop(columns=['TF', 'TG'])\n",
    "        # Remove duplicates\n",
    "        graphs[i] = graphs[i][~graphs[i].index.duplicated(keep='first')]\n",
    "\n",
    "    # Get all unique edges\n",
    "    print('Getting unique edges...')\n",
    "    all_edges = np.unique(sum([list(g.index) for g in graphs], []))\n",
    "\n",
    "\n",
    "    # Standardize index order\n",
    "    print('Standardizing indices...')\n",
    "    for i in tqdm(range(len(graphs))):\n",
    "        # Add missing indices and order based on `all_edges`\n",
    "        # to_add = [edge for edge in all_edges if edge not in list(graphs[i].index)]  # SLOW\n",
    "        to_add = list(set(all_edges) - set(graphs[i].index))\n",
    "\n",
    "        # Empty rows\n",
    "        new_rows = pd.DataFrame(\n",
    "            [[np.nan]*len(graphs[i].columns)]*len(to_add),\n",
    "            columns=graphs[i].columns,\n",
    "        ).set_index(pd.Series(to_add))\n",
    "        # Native concat\n",
    "        graphs[i] = pd.concat([graphs[i], new_rows]).loc[all_edges]\n",
    "\n",
    "    # Convert to numpy\n",
    "    graphs = [g.to_numpy() for g in graphs]\n",
    "    attention_stack = np.stack(graphs, axis=-1)\n",
    "    # attention_stack.shape = (Edge, Head, Subject)\n",
    "    # attention_stack.shape = (all_edges, columns, subject_ids)\n",
    "\n",
    "    # Save all data\n",
    "    all_data = {'data': attention_stack, 'edges': all_edges, 'heads': columns, 'subject_ids': subject_ids}\n",
    "    # np.savez('attentions.npz', **all_data)\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(\n",
    "            all_data,\n",
    "            f,\n",
    "            protocol=pickle.HIGHEST_PROTOCOL,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional useful parameters\n",
    "self_loops = [split_edge_string(s)[0] == split_edge_string(s)[1] for s in all_edges]\n",
    "self_loops = np.array(self_loops)\n",
    "# Remove self loops\n",
    "all_edges = all_edges[~self_loops]\n",
    "attention_stack = attention_stack[~self_loops]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available attention columns: ['AD_imp_1', 'AD_imp_2', 'SCZ_imp_1', 'SCZ_imp_2', 'data_imp_1', 'data_imp_2', 'data_imp_3', 'data_imp_4']\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "print(f'\\nAvailable attention columns: {get_attention_columns()}')\n",
    "column_ad = get_attention_columns()[0]\n",
    "column_scz = get_attention_columns()[2]\n",
    "column_data = get_attention_columns()[4]\n",
    "synthetic_nodes_of_interest = ['OPC', 'Micro', 'Oligo']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intra-Contrast Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure parameters\n",
    "param = {\n",
    "    'subjects': ['M31969', 'M20337'],\n",
    "    'columns': [column_data, column_ad, column_scz],\n",
    "    'column_names': ['Data-Driven', 'AD-Prior', 'SCZ-Prior'],\n",
    "    'column_groups': [get_attention_columns()[4:8], get_attention_columns()[:2], get_attention_columns()[2:4]],\n",
    "    'column_group_names': ['Data Prioritization', 'AD Prioritization', 'SCZ Prioritization'],\n",
    "    'ancestries': meta.groupby('Ethnicity').count()['SubID'].sort_values().index[::-1].to_list()[:3] + ['all'],\n",
    "    'contrast': 'c15x',\n",
    "}\n",
    "\n",
    "# Generate palette\n",
    "palette = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "param['palette'] = {sid: rgba_to_hex(palette[i]) for i, sid in enumerate(param['subjects'])}\n",
    "\n",
    "# Preview subjects\n",
    "for sid in param['subjects']:\n",
    "    row = meta.loc[meta['SubID']==sid].iloc[0]\n",
    "    # print(f'{row[\"SubID\"]} {row[\"Ethnicity\"]} {row[\"Sex\"]}, {row[\"Age\"]}, BRAAK {row[\"BRAAK_AD\"]}, CERAD {row[\"CERAD\"]}, CDR {row[\"CDRScore\"]}, {row[\"Dx\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot layout (doesn't work well with constrained layout)\n",
    "# NOTE: This cannot be used, as constrained layout has glitches\n",
    "# (see https://github.com/matplotlib/matplotlib/issues/23290)\n",
    "# with uneven mosaics\n",
    "# fig, axs = get_mosaic(shape, figsize=(int((3/2) * shape_array.shape[1]), int((3/2) * shape_array.shape[0])), constrained_layout=False)\n",
    "\n",
    "# Subfigure layout (longer)\n",
    "# NOTE: Constrained layout will fail for all\n",
    "# subplots if a single one is not able to scale.\n",
    "# Also, sometimes leaving a subfigure blank will\n",
    "# cause it to fail, especially if on an edge.\n",
    "# It is VERY finnicky.\n",
    "# SOLUTION: Save again using `fig.savefig(...)`\n",
    "# and it will run without warning.  Then, you\n",
    "# can visually inspect for scaling issues.\n",
    "# fig, axs = create_subfigure_mosaic(shape_array)\n",
    "# fig.set_constrained_layout_pads(w_pad=0, h_pad=0, wspace=.4, hspace=.4)  # *_pad is pad for figs (including subfigs), *_space is pad between subplots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Prioritization and Cross-Ancestry Enrichment - Figure 5cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = \"\"\"\n",
    "    NNNNNNNNNNN\n",
    "    NNNNNNNNNNN\n",
    "    NNNNNNNNNNN\n",
    "    NNNNNNNNNNN\n",
    "    RRRRRRRRRRR\n",
    "    RRRRRRRRRRR\n",
    "    RRRRRRRRRRR\n",
    "    RRRRRRRRRRR\n",
    "    RRRRRRRRRRR\n",
    "    RRRRRRRRRRR\n",
    "    RRRRRRRRRRR\n",
    "\"\"\"\n",
    "fig, axs = create_subfigure_mosaic(shape_array_from_shape(shape))\n",
    "\n",
    "axs_lab = (len(param['ancestries']) - 1) * ['None'] + ['N']\n",
    "# axs_lab = ['K', 'L', 'M', 'N']\n",
    "print(f'\\nEdge Discovery Enrichment ({\", \".join(axs_lab)})')\n",
    "for ancestry, ax in zip(param['ancestries'], [axs[lab] if lab in axs else None for lab in axs_lab]):\n",
    "    # Filter to ancestry\n",
    "    anc_data = all_data.copy()\n",
    "    if ancestry != 'all':\n",
    "        sub_ids = meta.loc[meta['Ethnicity'] == ancestry, 'SubID'].to_list()\n",
    "        mask = [sid in sub_ids for sid in anc_data['subject_ids']]\n",
    "        anc_data['data'] = anc_data['data'][:, :, mask]\n",
    "        anc_data['subject_ids'] = np.array(anc_data['subject_ids'])[mask]\n",
    "\n",
    "    # Run\n",
    "    temp = plot_edge_discovery_enrichment(\n",
    "        **anc_data,\n",
    "        column=param['columns'][0],\n",
    "        range_colors=[rgb_to_float(hex_to_rgb('#7aa457')), rgb_to_float(hex_to_rgb('#a46cb7')), rgb_to_float(hex_to_rgb('#cb6a49'))],\n",
    "        ax=ax,\n",
    "        postfix=f'{ancestry}_{param[\"columns\"][0]}',\n",
    "        gene_max_num=300,\n",
    "        threshold=95,\n",
    "        clamp_min=4,\n",
    "        skip_plot=(ax is None),\n",
    "        verbose=True)\n",
    "    if ax is not None:\n",
    "        ax.set_xlabel(f'High-Scoring Edges ({param[\"column_names\"][0]})')\n",
    "        ylabel = 'Frequency'\n",
    "        if ancestry != 'all': ylabel += f' ({ancestry})'\n",
    "        ax.set_ylabel(ylabel)\n",
    "    # MANUAL PROCESSING\n",
    "    # Run the output '../plots/genes_<column>.csv' from above on Metascape as multiple gene list and perform\n",
    "    # enrichment.  From the all-in-one ZIP file, save the file from Enrichment_GO/GO_membership.csv as '../plots/go_<column>.csv'\n",
    "    # and rerun.\n",
    "\n",
    "axs_lab = ['R']\n",
    "print(f'\\nAncestry Enrichment Comparison ({\", \".join(axs_lab)})')\n",
    "postfixes = [f'{ancestry}_{param[\"columns\"][0]}' for ancestry in param['ancestries']]\n",
    "enrichments = plot_cross_enrichment(postfixes, names=param['ancestries'], ax=axs[axs_lab[0]], excluded_subgroups=['all'])\n",
    "\n",
    "# Place labels\n",
    "offset = plot_labels(axs, shape=shape)\n",
    "\n",
    "# Save figure\n",
    "print('\\nSaving Figure...')\n",
    "fig.savefig(f'../plots/figure_5_main.pdf', bbox_inches='tight', pad_inches=1, format='pdf', transparent=True, backend='cairo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions Across Heads and Ancestries - Supplementary Figure 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = \"\"\"\n",
    "    KKKKKKKKKKKLLLLLLLLLLLMMMMMMMMMMM\n",
    "    KKKKKKKKKKKLLLLLLLLLLLMMMMMMMMMMM\n",
    "    KKKKKKKKKKKLLLLLLLLLLLMMMMMMMMMMM\n",
    "    KKKKKKKKKKKLLLLLLLLLLLMMMMMMMMMMM\n",
    "    KKKKKKKKKKKLLLLLLLLLLLMMMMMMMMMMM\n",
    "    KKKKKKKKKKKLLLLLLLLLLLMMMMMMMMMMM\n",
    "    KKKKKKKKKKKLLLLLLLLLLLMMMMMMMMMMM\n",
    "    KKKKKKKKKKKLLLLLLLLLLLMMMMMMMMMMM\n",
    "    KKKKKKKKKKKLLLLLLLLLLLMMMMMMMMMMM\n",
    "    KKKKKKKKKKKLLLLLLLLLLLMMMMMMMMMMM\n",
    "    KKKKKKKKKKKLLLLLLLLLLLMMMMMMMMMMM\n",
    "    NNNNNNNNNNNOOOOOOOOOOOPPPPPPPPPPP\n",
    "    NNNNNNNNNNNOOOOOOOOOOOPPPPPPPPPPP\n",
    "    NNNNNNNNNNNOOOOOOOOOOOPPPPPPPPPPP\n",
    "    NNNNNNNNNNNOOOOOOOOOOOPPPPPPPPPPP\n",
    "    NNNNNNNNNNNOOOOOOOOOOOPPPPPPPPPPP\n",
    "    NNNNNNNNNNNOOOOOOOOOOOPPPPPPPPPPP\n",
    "    NNNNNNNNNNNOOOOOOOOOOOPPPPPPPPPPP\n",
    "    NNNNNNNNNNNOOOOOOOOOOOPPPPPPPPPPP\n",
    "    NNNNNNNNNNNOOOOOOOOOOOPPPPPPPPPPP\n",
    "    NNNNNNNNNNNOOOOOOOOOOOPPPPPPPPPPP\n",
    "    NNNNNNNNNNNOOOOOOOOOOOPPPPPPPPPPP\n",
    "    QQQQQQQQQQQRRRRRRRRRRRSSSSSSSSSSS\n",
    "    QQQQQQQQQQQRRRRRRRRRRRSSSSSSSSSSS\n",
    "    QQQQQQQQQQQRRRRRRRRRRRSSSSSSSSSSS\n",
    "    QQQQQQQQQQQRRRRRRRRRRRSSSSSSSSSSS\n",
    "    QQQQQQQQQQQRRRRRRRRRRRSSSSSSSSSSS\n",
    "    QQQQQQQQQQQRRRRRRRRRRRSSSSSSSSSSS\n",
    "    QQQQQQQQQQQRRRRRRRRRRRSSSSSSSSSSS\n",
    "    QQQQQQQQQQQRRRRRRRRRRRSSSSSSSSSSS\n",
    "    QQQQQQQQQQQRRRRRRRRRRRSSSSSSSSSSS\n",
    "    QQQQQQQQQQQRRRRRRRRRRRSSSSSSSSSSS\n",
    "    QQQQQQQQQQQRRRRRRRRRRRSSSSSSSSSSS\n",
    "    TTTTTTTTTTTUUUUUUUUUUUVVVVVVVVVVV\n",
    "    TTTTTTTTTTTUUUUUUUUUUUVVVVVVVVVVV\n",
    "    TTTTTTTTTTTUUUUUUUUUUUVVVVVVVVVVV\n",
    "    TTTTTTTTTTTUUUUUUUUUUUVVVVVVVVVVV\n",
    "    TTTTTTTTTTTUUUUUUUUUUUVVVVVVVVVVV\n",
    "    TTTTTTTTTTTUUUUUUUUUUUVVVVVVVVVVV\n",
    "    TTTTTTTTTTTUUUUUUUUUUUVVVVVVVVVVV\n",
    "    TTTTTTTTTTTUUUUUUUUUUUVVVVVVVVVVV\n",
    "    TTTTTTTTTTTUUUUUUUUUUUVVVVVVVVVVV\n",
    "    TTTTTTTTTTTUUUUUUUUUUUVVVVVVVVVVV\n",
    "    TTTTTTTTTTTUUUUUUUUUUUVVVVVVVVVVV\n",
    "\"\"\"\n",
    "fig, axs = create_subfigure_mosaic(shape_array_from_shape(shape))\n",
    "matplotlib.rcParams['font.size'] = 45\n",
    "\n",
    "# Plot all panels\n",
    "axs_lab = ['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V']\n",
    "print(f'\\nEdge Discovery Enrichment ({\", \".join(axs_lab)})')\n",
    "for (ancestry, column_idx), ax in zip(product(param['ancestries'], range(len(param['columns']))), [axs[lab] if lab in axs else None for lab in axs_lab]):\n",
    "    # Filter to ancestry\n",
    "    anc_data = all_data.copy()\n",
    "    if ancestry != 'all':\n",
    "        sub_ids = meta.loc[meta['Ethnicity'] == ancestry, 'SubID'].to_list()\n",
    "        mask = [sid in sub_ids for sid in anc_data['subject_ids']]\n",
    "        anc_data['data'] = anc_data['data'][:, :, mask]\n",
    "        anc_data['subject_ids'] = np.array(anc_data['subject_ids'])[mask]\n",
    "\n",
    "    # Run\n",
    "    temp = plot_edge_discovery_enrichment(\n",
    "        **anc_data,\n",
    "        column=param['columns'][column_idx],\n",
    "        range_colors=[rgb_to_float(hex_to_rgb('#7aa457')), rgb_to_float(hex_to_rgb('#a46cb7')), rgb_to_float(hex_to_rgb('#cb6a49'))],\n",
    "        ax=ax,\n",
    "        postfix=f'{ancestry}_{param[\"columns\"][column_idx]}',\n",
    "        gene_max_num=300,\n",
    "        threshold=95,\n",
    "        wrap_chars=15,\n",
    "        skip_plot=(ax is None))\n",
    "    if ax is not None:\n",
    "        ax.set_xlabel(f'High-Scoring Edges ({param[\"column_names\"][column_idx]})')\n",
    "        ylabel = 'Frequency'\n",
    "        if ancestry != 'all': ylabel += f' ({ancestry})'\n",
    "        ax.set_ylabel(ylabel)\n",
    "    # MANUAL PROCESSING\n",
    "    # Run the output '../plots/genes_<column>.csv' from above on Metascape as multiple gene list and perform\n",
    "    # enrichment.  From the all-in-one ZIP file, save the file from Enrichment_GO/GO_membership.csv as '../plots/go_<column>.csv'\n",
    "    # and rerun.\n",
    "\n",
    "# Place labels\n",
    "offset = plot_labels(axs, shape=shape)\n",
    "\n",
    "# Save figure\n",
    "print('\\nSaving Figure...')\n",
    "fig.savefig(f'../plots/figure_5_supplement.pdf', bbox_inches='tight', pad_inches=1, format='pdf', transparent=True)  # , backend='cairo'\n",
    "matplotlib.rcParams['font.size'] = 22\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRS Analyses - Figure 6a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCZ\n",
    "# Data\n",
    "# AD\n",
    "# SCZ\n",
    "shape = \"\"\"\n",
    "    SSSSSSSSSSSSSSSUUUUUUUUUUUUUUU\n",
    "    SSSSSSSSSSSSSSSUUUUUUUUUUUUUUU\n",
    "    SSSSSSSSSSSSSSSUUUUUUUUUUUUUUU\n",
    "    SSSSSSSSSSSSSSSUUUUUUUUUUUUUUU\n",
    "\"\"\"\n",
    "fig, axs = create_subfigure_mosaic(shape_array_from_shape(shape))\n",
    "\n",
    "# Plot all panels\n",
    "axs_lab = ['S', 'U']\n",
    "print(f'\\nPRS Analysis ({\", \".join(axs_lab)})')\n",
    "# Takes around an hour for each loop with no subsampling (on first run)\n",
    "for fname, head_prefix, ylabel, prs_col, ax_idx in zip(\n",
    "    ('ad_prs_df.csv', 'scz_prs_df.csv'),\n",
    "    ('_'.join(column_ad.split('_')[:-1]), '_'.join(column_scz.split('_')[:-1])),\n",
    "    ('AD Importance Score', 'SCZ Importance Score'),\n",
    "    ('prs_scaled_AD_Bellenguez', 'prs_scaled_SCZ.3.5_MVP'),\n",
    "    axs_lab\n",
    "):\n",
    "    df = pd.read_csv(fname, index_col=0) if os.path.isfile(fname) else None\n",
    "    covariates = get_genotype_meta()[['SubID', 'imp_sex_score'] + [f'imp_anc_PC{i}' for i in range(1, 7)] + [f'imp_anc_{anc}' for anc in ('AFR', 'AMR', 'EAS', 'EUR')]]\n",
    "    df, prs_df, axs[ax_idx] = plot_prs_correlation(\n",
    "        meta, **all_data, ax=axs[ax_idx],\n",
    "        df=df, num_targets=5, ylabel=ylabel, max_scale=False,\n",
    "        head_prefix=head_prefix, prs_col=prs_col,\n",
    "        covariates=covariates, subsample=1)\n",
    "    if not os.path.isfile(fname): df.to_csv(fname)\n",
    "\n",
    "# Place labels\n",
    "offset = plot_labels(axs, shape=shape)\n",
    "\n",
    "# Save figure\n",
    "print('\\nSaving Figure...')\n",
    "fig.savefig(f'../plots/figure_6_prs.pdf', bbox_inches='tight', pad_inches=1, format='pdf', transparent=True, backend='cairo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revision Panels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revision Gene Importance Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "importance_scores = get_importance_scores()\n",
    "importance_scores = importance_scores.rename(columns={'ad_imp_score': 'AD', 'scz_imp_score': 'SCZ', 'data_imp_score': 'Data'})\n",
    "importance_scores['All'] = (importance_scores['AD'] * 2 + importance_scores['SCZ'] * 2 + importance_scores['Data'] * 4) / 8\n",
    "\n",
    "# Get meta\n",
    "importance_scores['BRAAK_AD'] = meta.set_index('SubID').loc[importance_scores['sample'], 'BRAAK_AD'].to_numpy()\n",
    "\n",
    "# Calculate correlations\n",
    "heads = ['All']  # ['AD', 'SCZ', 'Data']\n",
    "df_corr = pd.DataFrame(columns=['Gene', 'Head', 'Correlation', 'Significance', 'Samples'])\n",
    "unique_genes = importance_scores['node'].unique()\n",
    "for i, g in tqdm(enumerate(unique_genes), total=unique_genes.shape[0]):\n",
    "    for j, h in enumerate(heads):\n",
    "        filtered_df = importance_scores.loc[(importance_scores['node'] == g)]\n",
    "        col = filtered_df[h].to_numpy()\n",
    "        geno = filtered_df['BRAAK_AD'].to_numpy()\n",
    "        mask = ~np.isnan(col) * ~np.isnan(geno)\n",
    "        if mask.sum() > 2 and col[mask].var() != 0 and geno[mask].var() != 0:\n",
    "            corr, sig = scipy.stats.spearmanr(col[mask], geno[mask])\n",
    "            num = mask.sum()\n",
    "            df_corr.loc[df_corr.shape[0]] = {'Gene': g, 'Head': h, 'Correlation': corr, 'Significance': sig, 'Samples': num}\n",
    "\n",
    "# FDR correction\n",
    "df_corr['Adjusted Significance'] = scipy.stats.false_discovery_control(df_corr['Significance'].clip(0, 1), method='bh')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "min_samples = 5\n",
    "significance_threshold = 5e-2\n",
    "top_genes = int(1e3)\n",
    "\n",
    "# Histogram of adjusted significance\n",
    "axs = df_corr.plot.hist(column=['Significance', 'Adjusted Significance'], by='Head', bins=int(2/significance_threshold), figsize=(12, 5*df_corr['Head'].unique().shape[0]))\n",
    "for ax in axs:\n",
    "    ax.set_yscale('log')\n",
    "    ax.axvline(x=significance_threshold, ls='--', color='red')\n",
    "plt.savefig(f'../plots/rev_1_histogram.pdf', bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Separate head analysis\n",
    "# # Gene lists\n",
    "# sig_genes = pd.DataFrame()\n",
    "# for h in heads:\n",
    "#     # Filter dataframe\n",
    "#     df_filtered = df\n",
    "#     df_filtered = df_filtered.loc[df_filtered['Head'] == h]  # Head\n",
    "#     df_filtered = df_filtered.loc[df_filtered['Samples'] > min_samples]  # Samples\n",
    "#     df_filtered = df_filtered.loc[df_filtered['Adjusted Significance'] < significance_threshold]  # Significance\n",
    "#     genes = np.unique([g.split(':')[1] for g in df_filtered[['Gene']].to_numpy().flatten() if g.split(':')[0] in ('TF', 'TG')])\n",
    "\n",
    "#     # Save genes\n",
    "#     print(f'Significant {h} genes: {genes.shape[0]}')\n",
    "#     sig_genes = pd.concat([sig_genes, pd.DataFrame({h: genes})], axis=1)\n",
    "\n",
    "# # Add background and save\n",
    "# genes = np.unique([g.split(':')[1] for g in importance_scores[['node']].to_numpy().flatten() if g.split(':')[0] in ('TF', 'TG')])\n",
    "# genes = np.array([g for g in genes if not string_is_synthetic(g)])\n",
    "# sig_genes = pd.concat([sig_genes, pd.DataFrame({'_BACKGROUND': genes})], axis=1)\n",
    "# pd.DataFrame(sig_genes).to_csv('../plots/rev_1_genes.csv', index=False)\n",
    "# # np.savetxt(f'../plots/rev_1_genes_{h.lower()}.txt', genes, fmt='%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate analysis\n",
    "# Gene lists\n",
    "sig_genes = {}\n",
    "\n",
    "# Filter for positive and negative\n",
    "for name, factor in zip(('Positive', 'Negative'), (1, -1)):\n",
    "    df_filtered = df_corr\n",
    "    df_filtered = df_filtered.loc[df_filtered['Samples'] >= min_samples]  # Samples\n",
    "    df_filtered = df_filtered.loc[df_filtered['Adjusted Significance'] <= significance_threshold]  # Significance\n",
    "    df_filtered = df_filtered.loc[factor * df_filtered['Correlation'] > 0]  # Correlation parity\n",
    "    df_filtered = df_filtered.sort_values(by='Correlation', ascending=factor<0)  # Sort genes\n",
    "    df_filtered['Gene'] = df_filtered['Gene'].apply(lambda g: g.split(':')[1] if g.split(':')[0] in ('TF', 'TG') else pd.NA)  # Remove celltypes\n",
    "    df_filtered = df_filtered.dropna()\n",
    "    df_filtered = df_filtered[~df_filtered['Gene'].duplicated(keep='first')]  # Remove TF + TG genes that got in twice, keep higher score\n",
    "    df_filtered = df_filtered.iloc[:top_genes]\n",
    "    genes = df_filtered['Gene'].to_numpy()\n",
    "    corr = df_filtered['Correlation'].to_numpy()\n",
    "    sig = df_filtered['Significance'].to_numpy()\n",
    "\n",
    "    # Save genes\n",
    "    print(f'Post-filter {name} genes: {genes.shape[0]}')  # Post-filter\n",
    "    sig_genes[name] = df_filtered[['Gene', 'Samples', 'Correlation', 'Significance', 'Adjusted Significance']]\n",
    "\n",
    "# Add background and save\n",
    "genes = np.unique([g.split(':')[1] for g in importance_scores[['node']].to_numpy().flatten() if g.split(':')[0] in ('TF', 'TG')])\n",
    "background = pd.DataFrame({'_BACKGROUND': genes})\n",
    "# sig_genes.to_csv('../plots/rev_1_genes.csv', index=False)\n",
    "with pd.ExcelWriter('../plots/rev_1_genes.xlsx') as w:\n",
    "    for name, df in sig_genes.items():\n",
    "        df.to_excel(w, sheet_name=name, index=False)\n",
    "    background.to_excel(w, sheet_name='Background', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revision Ancestry Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data-driven column and ancestries\n",
    "dd_col = param['columns'][0]\n",
    "ancs = param['ancestries'][:-1]\n",
    "\n",
    "# Load high-percentile gene sets\n",
    "fnames = [f'../plots/genes_{anc}_{dd_col}.csv' for anc in ancs]\n",
    "dfs = [pd.read_csv(fname) for fname in fnames]\n",
    "data = [df.iloc[:, -2].dropna().sort_values().to_list() for df in dfs]\n",
    "anc_sizes = [len(d) for d in data]\n",
    "\n",
    "# Find intersections\n",
    "from collections import defaultdict\n",
    "def find_intersections(data, names):\n",
    "    groups = defaultdict(lambda: [])\n",
    "    while sum([len(d) for d in data]) > 0:\n",
    "        # Query current values\n",
    "        low_idx = np.nanargmin([d[0] if len(d) > 0 else np.nan for d in data])\n",
    "\n",
    "        # Pop\n",
    "        low_val = data[low_idx].pop(0)\n",
    "\n",
    "        # Find and pop other equal heads\n",
    "        other_equal = [i for i, d in enumerate(data) if i != low_idx and len(d) > 0 and d[0] == low_val]\n",
    "        for i in other_equal: data[i].pop(0)\n",
    "\n",
    "        # Aggregate intersection and record\n",
    "        all_equal = np.sort(other_equal + [low_idx])\n",
    "        groups['-'.join(np.sort([names[i] for i in all_equal]))].append(low_val)\n",
    "    \n",
    "    return groups\n",
    "groups = find_intersections(data, ancs)\n",
    "\n",
    "# Print group sizes\n",
    "# groups = dict(groups)\n",
    "print('Intersection counts')\n",
    "for k, v in groups.items():\n",
    "    print(f'{k}: {len(v)}')\n",
    "\n",
    "# Save unique genes\n",
    "new_fnames = [f'../plots/genes_{k}_{dd_col}_uniq.csv' for k in groups]\n",
    "for new_fname, k in zip(new_fnames, groups): pd.concat([\n",
    "        pd.DataFrame({dfs[0].columns[-2]: groups[k]}),\n",
    "        pd.DataFrame({'_BACKGROUND': dfs[0]['_BACKGROUND']})\n",
    "    ], axis=1).to_csv(new_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate overlap\n",
    "np.random.seed(42)\n",
    "num_gene_sets = [\n",
    "    ('All Background', dfs[0]['_BACKGROUND'].shape[0]),  # Assumes bachground is the same between all\n",
    "    ('Highly Variant', np.unique([g for grp in groups.values() for g in grp])),  # Only variant genes from each ancestry\n",
    "]\n",
    "group_sizes = anc_sizes\n",
    "num_iterations = int(1e4)\n",
    "sim_group_counts = defaultdict(lambda: defaultdict(lambda: []))\n",
    "for background_name, num_genes in num_gene_sets:\n",
    "    for _ in tqdm(range(num_iterations), total=num_iterations, desc=f'{background_name}'):\n",
    "        # Sample gene lists\n",
    "        sim_data = []\n",
    "        for size in group_sizes:\n",
    "            sim_data.append( np.sort(np.random.choice(num_genes, size, replace=False)).tolist() )\n",
    "        \n",
    "        # Find intersections\n",
    "        sim_groups = find_intersections(sim_data, ancs)\n",
    "\n",
    "        # Record\n",
    "        for k, v in sim_groups.items():\n",
    "            sim_group_counts[background_name][k].append(len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each sim group with the corresponding reading\n",
    "group_order = ['AFR', 'AMR', 'EUR', 'AFR-AMR', 'AFR-EUR', 'AMR-EUR', 'AFR-AMR-EUR']\n",
    "fig, axs = plt.subplots(len(num_gene_sets), len(groups), figsize=(len(groups)*8, len(num_gene_sets)*4))\n",
    "for i, k1 in enumerate(sim_group_counts):\n",
    "    for j, k2 in enumerate(group_order):\n",
    "        sim_count = sim_group_counts[k1][k2]\n",
    "        obs_count = len(groups[k2])\n",
    "        quantile = (np.array(sim_count) < obs_count).mean()\n",
    "\n",
    "        # Plot\n",
    "        ax = axs[i][j]\n",
    "        data = pd.DataFrame({k: sim_group_counts[k1][k2]})\n",
    "        sns.histplot(data=data, x=k, kde=True, color='gray', ax=ax)\n",
    "\n",
    "        # Significance\n",
    "        ax.axvline(x=obs_count, ls='-', color='red')\n",
    "        pval = min(quantile, 1-quantile)  # One-tailed p test\n",
    "        if pval < 1/num_iterations: pval = 1/num_iterations\n",
    "        sig_thresholds = np.array([5e-2, 1e-2, 1e-3])\n",
    "        sig_tail = '*' * (pval < sig_thresholds).sum()\n",
    "        pval_string = f'p<{pval:.2e}{sig_tail}'\n",
    "        ax.text(.5, .95, pval_string, ha='center', va='top', transform=ax.transAxes)\n",
    "\n",
    "        # Labels\n",
    "        ax.set(xlabel=None, ylabel=None)\n",
    "        if i == 0: ax.set_title(k2)\n",
    "        if j == 0: ax.set_ylabel(k1)\n",
    "fig.savefig(f'../plots/rev_1_ancestry_significance.pdf', bbox_inches='tight', transparent=True)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot after MANUAL PROCESSING\n",
    "# TODO: Maybe also plot intersecting genes?\n",
    "fig, ax = plt.subplots(1, 1, figsize=(int((3/2)*11), int((3/2)*7)))\n",
    "file_prefixes = ['_'.join(fname.split('/')[-1].split('_')[1:])[:-4] for fname, k in zip(new_fnames, groups) if len(groups[k]) > 100]\n",
    "group_names = [fname.split('_')[0] for fname in file_prefixes]\n",
    "enrichments = plot_cross_enrichment(file_prefixes, names=group_names, num_terms=30, ax=ax)\n",
    "fig.savefig(f'../plots/rev_1_ancestry.pdf', bbox_inches='tight', transparent=True)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary Revision Panels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms of Edge Prioritization Distributions - Supplementary Figure XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = \"\"\"\n",
    "    KKKKKKKLLLLLLLMMMMMMM\n",
    "    KKKKKKKLLLLLLLMMMMMMM\n",
    "    KKKKKKKLLLLLLLMMMMMMM\n",
    "    NNNNNNNOOOOOOOPPPPPPP\n",
    "    NNNNNNNOOOOOOOPPPPPPP\n",
    "    NNNNNNNOOOOOOOPPPPPPP\n",
    "    QQQQQQQRRRRRRRSSSSSSS\n",
    "    QQQQQQQRRRRRRRSSSSSSS\n",
    "    QQQQQQQRRRRRRRSSSSSSS\n",
    "    TTTTTTTUUUUUUUVVVVVVV\n",
    "    TTTTTTTUUUUUUUVVVVVVV\n",
    "    TTTTTTTUUUUUUUVVVVVVV\n",
    "\"\"\"\n",
    "fig, axs = create_subfigure_mosaic(shape_array_from_shape(shape))\n",
    "matplotlib.rcParams['font.size'] = 45\n",
    "\n",
    "# Plot all panels\n",
    "axs_lab = ['K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V']\n",
    "print(f'\\nEdge Discovery Enrichment ({\", \".join(axs_lab)})')\n",
    "for (ancestry, column_idx), ax in zip(product(param['ancestries'], range(len(param['columns']))), [axs[lab] if lab in axs else None for lab in axs_lab]):\n",
    "    # Filter to ancestry\n",
    "    anc_data = all_data.copy()\n",
    "    if ancestry != 'all':\n",
    "        sub_ids = meta.loc[meta['Ethnicity'] == ancestry, 'SubID'].to_list()\n",
    "        mask = [sid in sub_ids for sid in anc_data['subject_ids']]\n",
    "        anc_data['data'] = anc_data['data'][:, :, mask]\n",
    "        anc_data['subject_ids'] = np.array(anc_data['subject_ids'])[mask]\n",
    "\n",
    "    # Run\n",
    "    # Compute edge counts\n",
    "    edge_counts = compute_edge_counts(**anc_data)\n",
    "    head_filt = edge_counts['Head'] == param['columns'][column_idx]\n",
    "    zero_filt = edge_counts['Count'] > 0\n",
    "    edge_counts_filt = edge_counts.loc[head_filt * zero_filt]\n",
    "    # Plot histogram\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    sns.histplot(\n",
    "        edge_counts_filt, x='Count', bins=.5+np.concatenate([np.linspace(0, 10, 11), np.floor(np.logspace(1, 3, 21))[1:]]),\n",
    "        color='gray', edgecolor='.3', lw=.5, ax=ax)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set(xlabel=f'Prioritizations ({param[\"column_names\"][column_idx]})', ylabel=f'Edges ({ancestry})')\n",
    "    # Axes\n",
    "    ax.set_yscale('log', subs=list(range(2, 10)))\n",
    "    ax.set_xscale('symlog', linthresh=10)\n",
    "    ax.set_xlim(left=0)\n",
    "    # Ticks\n",
    "    ax.yaxis.get_major_locator().set_params(numticks=99)\n",
    "    ax.yaxis.get_minor_locator().set_params(numticks=99)\n",
    "    ax.set_xticks(np.concatenate([[0, 1]] + [np.linspace(10**i, 10**(i+1), 10)[1:] for i in range(3)]), minor=True)\n",
    "    ax.tick_params(which='both', left=True, bottom=True)\n",
    "\n",
    "# Place labels\n",
    "offset = plot_labels(axs, shape=shape)\n",
    "\n",
    "# Save figure\n",
    "print('\\nSaving Figure...')\n",
    "fig.savefig(f'../plots/figure_5_supplement_hist.pdf', bbox_inches='tight', pad_inches=1, format='pdf', transparent=True)  # , backend='cairo'\n",
    "matplotlib.rcParams['font.size'] = 22\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Edge Enrichments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all present edges\n",
    "has_value = np.argwhere(~np.isnan(all_data['data']))\n",
    "present_edges = pd.DataFrame({\n",
    "    'Edges': all_data['edges'][has_value[:, 0]],\n",
    "    'Heads': np.array(all_data['heads'])[has_value[:, 1]],\n",
    "    'Subject IDs': np.array(all_data['subject_ids'])[has_value[:, 2]],\n",
    "    'Value': all_data['data'][has_value[:, 0], has_value[:, 1], has_value[:, 2]]})\n",
    "present_edges[['Source', 'Target']] = present_edges['Edges'].str.split(EDGE_SPLIT_STRING, expand=True)\n",
    "\n",
    "# Apply any pre-filtering\n",
    "# present_edges = present_edges.loc[present_edges['Value'] > present_edges['Value'].quantile(.9)]  # 90th percentile of attentions BEFORE assessing clusters for uniqueness\n",
    "\n",
    "# Attach clusters\n",
    "clusters = pd.read_csv('./figure_2d_clusters.csv', index_col=0)\n",
    "present_edges = present_edges.join(clusters.set_index('SubID')[['louvain']], on='Subject IDs').dropna()  # Some subjects are not found in the clustering\n",
    "instances_edges_cluster_series = present_edges.groupby(['Edges', 'louvain']).size() / len(all_data['heads'])  # How many per edge and cluster\n",
    "instances_edges_series = instances_edges_cluster_series.reset_index().groupby('Edges').size()  # How many clusters per edge\n",
    "unique_edges = instances_edges_series.index[instances_edges_series == 1]  # Get cluster-unique edges\n",
    "unique_present_edges = present_edges.loc[present_edges['Edges'].isin(unique_edges)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 has 462 unique edges\n",
      "Cluster 1 has 358 unique edges\n",
      "Cluster 2 has 342 unique edges\n",
      "Cluster 3 has 189 unique edges\n",
      "Cluster 4 has 47 unique edges\n"
     ]
    }
   ],
   "source": [
    "# Params\n",
    "head = 'data'\n",
    "\n",
    "# Apply any post-filtering\n",
    "if '_' not in head: unique_present_edges_filt = unique_present_edges.loc[unique_present_edges['Heads'].apply(lambda s: s.startswith(head))]  # Filter to group of heads\n",
    "else: unique_present_edges_filt = unique_present_edges.loc[unique_present_edges['Heads'] == head]  # Filter to individual head\n",
    "unique_present_edges_filt = unique_present_edges_filt.drop(columns='Heads').groupby(['Edges', 'Subject IDs', 'Source', 'Target', 'louvain']).mean().reset_index()  # Mean over all remaining heads\n",
    "unique_present_edges_filt = unique_present_edges_filt.loc[unique_present_edges_filt['Value'] > present_edges['Value'].quantile(.98)]  # 98th percentile of attentions AFTER assessing clusters for uniqueness\n",
    "\n",
    "# CLI\n",
    "for cluster, num in unique_present_edges_filt.groupby('louvain').size().items():\n",
    "    print(f'Cluster {cluster:.0f} has {num:.0f} unique edges')  # NOTE: If multiple heads, might over-count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 has 361 genes within unique edges\n",
      "Cluster 1 has 300 genes within unique edges\n",
      "Cluster 2 has 282 genes within unique edges\n",
      "Cluster 3 has 167 genes within unique edges\n",
      "Cluster 4 has 47 genes within unique edges\n"
     ]
    }
   ],
   "source": [
    "# Get unique genes by cluster\n",
    "unique_genes = {'_BACKGROUND': pd.Series(pd.concat([present_edges['Source'], present_edges['Target']]).unique())}\n",
    "for cluster in unique_present_edges_filt['louvain'].unique():\n",
    "    df = unique_present_edges_filt\n",
    "    df = df.loc[df['louvain'] == cluster]\n",
    "    unique_gene_list = pd.Series(pd.concat([df['Source'], df['Target']]).unique())\n",
    "    unique_gene_list = unique_gene_list[~unique_gene_list.apply(string_is_synthetic)].reset_index(drop=True)\n",
    "    unique_genes[cluster] = unique_gene_list\n",
    "    print(f'Cluster {cluster:.0f} has {unique_genes[cluster].shape[0]:.0f} genes within unique edges')\n",
    "unique_genes = pd.DataFrame(unique_genes)\n",
    "\n",
    "# Save\n",
    "unique_genes.to_csv(f'../plots/cluster_unique_genes_{head}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN IN METASCAPE AND EXTRACT `GO_membership.csv`, rename to `cluster_unique_genes_{head}_GO.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "head = 'data'  # data_imp_1, data, SCZ, AD\n",
    "\n",
    "# Construct data\n",
    "enrichment = pd.read_csv(f'../plots/cluster_unique_genes_{head}_GO.csv', index_col=None)\n",
    "cluster_cols = [f'_LogP_{i:.1f}' for i in range(5)]\n",
    "filtered_enrichment = enrichment.loc[enrichment[cluster_cols].mean(axis=1).argsort()].groupby('_PATTERN_').head(3)  # 3 highest means for each pattern\n",
    "df = -filtered_enrichment.set_index('Description')[cluster_cols].rename(columns={col: i for i, col in enumerate(cluster_cols)}).iloc[::-1]\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(10, 20))\n",
    "\n",
    "# Create grid\n",
    "xlabels = df.columns\n",
    "ylabels = df.index\n",
    "Y, X = np.meshgrid(np.arange(len(ylabels)), np.arange(len(xlabels)), indexing='ij')\n",
    "R = df.to_numpy() / df.to_numpy().max() / 2\n",
    "\n",
    "# Populate axis\n",
    "circles = [plt.Circle((x, y), radius=r) for x, y, r in zip(X.flat, Y.flat, R.flat)]\n",
    "col = matplotlib.collections.PatchCollection(circles, edgecolor='none', array=df.to_numpy().flat, cmap='Reds')  # cmap\n",
    "col.set_clim(vmin=0)\n",
    "ax.add_collection(col)\n",
    "\n",
    "# Formatting\n",
    "ax.set(\n",
    "    xticks=np.arange(len(xlabels)), xticklabels=xlabels, xlabel='Development Cluster',\n",
    "    yticks=np.arange(len(ylabels)), yticklabels=ylabels)\n",
    "ax.set_xticks(np.arange(len(xlabels)+1)-.5, minor=True)\n",
    "ax.set_yticks(np.arange(len(ylabels)+1)-.5, minor=True)\n",
    "ax.grid(which='minor')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "fig.colorbar(col, label='-Log10(p)')\n",
    "fig.savefig(f'../plots/cluster_unique_genes_{head}.pdf', bbox_inches='tight', pad_inches=1, format='pdf', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print gene list\n",
    "# cluster = 0\n",
    "# for g in unique_genes[cluster].dropna(): print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant Edge List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get significant DEX genes\n",
    "dex_genes = pd.read_csv('PsychAD_SupplementaryTable6.csv.gz')\n",
    "dex_gene_list = dex_genes.loc[dex_genes['FDR'] < .05, 'ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all present edges\n",
    "has_value = np.argwhere(~np.isnan(all_data['data']))\n",
    "present_edges = pd.DataFrame({\n",
    "    'Edge': all_data['edges'][has_value[:, 0]],\n",
    "    'Head': np.array(all_data['heads'])[has_value[:, 1]],\n",
    "    'Subject ID': np.array(all_data['subject_ids'])[has_value[:, 2]],\n",
    "    'Value': all_data['data'][has_value[:, 0], has_value[:, 1], has_value[:, 2]]})\n",
    "present_edges[['Source', 'Target']] = present_edges['Edge'].str.split(EDGE_SPLIT_STRING, expand=True)  # Extract genes\n",
    "present_edges = present_edges.join(meta.set_index('SubID')[['AD']], on='Subject ID')  # Append AD annotation\n",
    "\n",
    "# Get overlaps (total, not by ct)\n",
    "# TODO: Add CT\n",
    "present_edges['DEX Overlap'] = present_edges['Source'].isin(dex_gene_list) + present_edges['Target'].isin(dex_gene_list)\n",
    "\n",
    "# Get data means\n",
    "data_edges = present_edges.loc[present_edges['Head'].str.startswith('AD_imp_')].groupby(list(np.setxor1d(present_edges.columns, ['Value']))).mean().reset_index()\n",
    "\n",
    "# Filter\n",
    "data_edges['Top'] = data_edges['Value'] >= data_edges['Value'].quantile(.98)\n",
    "sig_edges = data_edges.loc[data_edges['Top']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108/2681865516.py:18: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEX Overlap</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hash</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Filtered</th>\n",
       "      <td>0.713676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-Filtered</th>\n",
       "      <td>0.668606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DEX Overlap\n",
       "Hash                     \n",
       "Filtered         0.713676\n",
       "Non-Filtered     0.668606"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get percentage shared with capstone\n",
    "# shared_w_capstone = data_edges[['Subject ID', 'AD', 'DEX Overlap', 'Top']].groupby(['Subject ID', 'AD', 'Top']).mean().reset_index()\n",
    "# shared_w_capstone['Hash'] = shared_w_capstone.apply(lambda r: f'{\"Filtered \" if r[\"Top\"] else \"\"}{\"AD\" if r[\"AD\"] else \"Non-AD\"}', axis=1)\n",
    "# Hard filter\n",
    "shared_w_capstone = data_edges[['Subject ID', 'AD', 'DEX Overlap', 'Top']].groupby(['Subject ID', 'AD', 'Top']).mean().reset_index()\n",
    "shared_w_capstone['Hash'] = shared_w_capstone.apply(lambda r: f'{\"Filtered\" if r[\"Top\"] else \"Non-Filtered\"}', axis=1)\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(3, 5))\n",
    "\n",
    "# Plot\n",
    "sns.boxplot(\n",
    "    shared_w_capstone, x='Hash', y='DEX Overlap',\n",
    "    # order=['Non-AD', 'Filtered Non-AD', 'AD', 'Filtered AD'], palette={0.: 'blue', 1.: 'red'}, hue='AD',\n",
    "    order=['Non-Filtered', 'Filtered'], color='black',\n",
    "    fill=False, legend=False, ax=ax)\n",
    "ax.set(xlabel=None, ylabel='Capstone Overlap')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "# ax.set(xticks=[0, 1], xticklabels=['Non-AD', 'AD'])\n",
    "ax.set_ylim(0, 1)\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "# Save\n",
    "fig.savefig(f'../plots/capstone_overlap.pdf', bbox_inches='tight', pad_inches=1, format='pdf', transparent=True)\n",
    "shared_w_capstone[['Hash', 'DEX Overlap']].groupby(['Hash']).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AD genes, include duplicates\n",
    "identified_genes = np.concatenate([sig_edges['Source'], sig_edges['Target']])\n",
    "identified_genes = np.array([g for g in identified_genes if not string_is_synthetic(g)])\n",
    "\n",
    "# Get AD genes, exclude duplicates\n",
    "# identified_genes = []\n",
    "# for sid in tqdm(sig_edges['Subject ID'].unique()):\n",
    "#     df_filt = sig_edges.loc[sig_edges['Subject ID']==sid]\n",
    "#     new_genes = np.concatenate([df_filt['Source'], df_filt['Target']])\n",
    "#     new_genes = np.unique(new_genes)\n",
    "#     identified_genes.append(new_genes)\n",
    "# identified_genes = np.concatenate(identified_genes)\n",
    "\n",
    "# Sort genes by frequency\n",
    "unique_genes, unique_counts = np.unique(identified_genes, return_counts=True)\n",
    "highest_counts = np.argsort(unique_counts)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve\n",
    "import sklearn.metrics\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(np.isin(unique_genes, dex_gene_list), unique_counts)\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Plot\n",
    "ax.plot(fpr, tpr, color='black')\n",
    "ax.plot([0, 1], [0, 1], ls='--', color='black')\n",
    "ax.set(title='Consensus ROC', xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
    "ax.text(.95, .05, f'AUC: {sklearn.metrics.auc(fpr, tpr):.3f}', ha='right', va='bottom', transform=ax.transAxes)\n",
    "for i in range(20, len(thresholds)-10, 45):\n",
    "    ax.text(fpr[i]+.07, tpr[i], f'{thresholds[i]:.0f}', ha='left', va='center', fontsize='xx-small', backgroundcolor='white', transform=ax.transData)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Save\n",
    "fig.savefig(f'../plots/capstone_recovery.pdf', bbox_inches='tight', pad_inches=1, format='pdf', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 236/236 [00:36<00:00,  6.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get overlap by population threshold\n",
    "results = []\n",
    "for thresh in tqdm(np.unique(unique_counts)):\n",
    "    gene_list = unique_genes[unique_counts >= thresh]\n",
    "    results.append({\n",
    "        'Gene Prioritization Count': thresh,\n",
    "        'Population Percentage': 1 - thresh / sig_edges['Subject ID'].unique().shape[0],  # Doesn't work with duplicates\n",
    "        'Gene Count': gene_list.shape[0],\n",
    "        'Overlap Percentage': np.isin(gene_list, dex_gene_list).mean(),\n",
    "    })\n",
    "recovery_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Plot\n",
    "sns.lineplot(recovery_df, x='Gene Prioritization Count', y='Overlap Percentage', color='black')\n",
    "# ax.set_xlim(0, 1)\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(0, 1.02)\n",
    "ax.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(1.))\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "# Annotate\n",
    "for thresh in np.arange(.6, 1.01, .1):\n",
    "    df_filt = recovery_df.loc[recovery_df['Overlap Percentage'] >= thresh]\n",
    "    argmin = df_filt['Gene Prioritization Count'].argmin()\n",
    "    x_pos, y_pos = df_filt.iloc[argmin]['Gene Prioritization Count'], df_filt.iloc[argmin]['Overlap Percentage']\n",
    "    x_offset, y_offset = x_pos+.02*ax.get_xlim()[1], y_pos-.13\n",
    "    gene_count = df_filt.iloc[argmin]['Gene Count']\n",
    "    ax.scatter(x_pos, y_pos, marker='.', s=200, color='black')\n",
    "    ax.axvline(x=x_pos, ymin=y_offset, ymax=y_pos, color='black', alpha=.7, ls='--')\n",
    "    # ({np.floor(100*y_pos):.0f}%)\n",
    "    ax.text(x_offset, y_offset, f'{x_pos:.0f} ({gene_count:.0f} Genes)', ha='left', va='bottom', transform=ax.transData)  # , bbox=dict(facecolor='white', edgecolor='none')\n",
    "\n",
    "# Save\n",
    "fig.savefig(f'../plots/capstone_recovery_overlap.pdf', bbox_inches='tight', pad_inches=1, format='pdf', transparent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
